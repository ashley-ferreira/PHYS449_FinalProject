{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashley-ferreira/PHYS449_FinalProject/blob/main/CNN_Fully_Augmented_Dataset_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PHYS 449: Final Project Notebook**\n",
        "#### Reproducing results from \"Morphological classification of galaxies with deep learning: comparing 3-way and 4-way CNNs\" by Mitchell K. Cavanagh, Kenji Bekki and Brent A. Groves\n",
        "\n",
        "Use Pytorch on Fully Augmented Dataset.\n",
        "Use C1 and C2."
      ],
      "metadata": {
        "id": "oQfh4ENuykfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set Current Working Directory**\n",
        "\n",
        "For example, for Ashley this is:\n",
        "\n",
        "'/content/drive/MyDrive/Fall 2022/PHYS 449/Final Project'"
      ],
      "metadata": {
        "id": "q-Ma_H5RnGtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CWD = '/content/drive/MyDrive/' #Jordan's current working directory\n",
        "from google.colab import drive #mount google drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Aa8SwGc2rpn1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94d12e51-0867-4a3a-d29e-bf5b7a6c81df"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Packages**\n",
        "\n",
        "Begin by importing all the needed packages"
      ],
      "metadata": {
        "id": "FvlSR1N2ykL9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FnQlrR-O-Jmh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define Network Structure**\n",
        "We are considering two 2D CNNs, C1 and C2, which are described in the paper and outlined below"
      ],
      "metadata": {
        "id": "Va-_oVtsykW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 4 #set the number of classes for the model"
      ],
      "metadata": {
        "id": "TtWL4NQip11I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#C1\n",
        "networkc1 = nn.Sequential(\n",
        "    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=1, padding=0),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=0),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "    nn.Flatten(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(135424,256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, num_classes))"
      ],
      "metadata": {
        "id": "ZOk2Z6iXJF1y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#C2\n",
        "networkc2 = nn.Sequential(\n",
        "    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=7, stride=1, padding=0),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=0),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(64),\n",
        "\n",
        "    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=5, stride=1, padding=0),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "    nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(8192, 256),\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256,256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, num_classes))"
      ],
      "metadata": {
        "id": "JetZ8Dimne4H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyB4yRaICITu"
      },
      "source": [
        "# **Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZmT2udOl-dlB"
      },
      "outputs": [],
      "source": [
        "#LOAD THE DATA FROM TXT FILE INTO A BATCH:\n",
        "def data_batch(datafile_index, num_images=10, data_file=CWD+'/data/data_g_band_v2.txt', plotting=False):\n",
        "    '''\n",
        "    Description:\n",
        "        Access datafile.txt, each row is flattened 110x110 image + 1 label string (E, Sp, S0, Irr+Misc).\n",
        "        Returns an augmented batch of num_images X 40.\n",
        "        The labels are converted to 1D vectors (ex: Sp = [0,0,1,0])\n",
        "        Need to give a datafile_index that tells which rows to pick.\n",
        "    Inputs:\n",
        "        datafile_index: index of row in datafile to load. loads rows datafile_index to datafile_index+num_images.\n",
        "        num_images: number of different images to load per batch, total batch size \n",
        "        is 40 X num_images. (default: 10 (for 40X10 = 400 batch size like in paper)\n",
        "        data_file: datafile full path, need to add shortcut to local Drive. (default: '/content/drive/MyDrive/data/data_g_band.txt')\n",
        "    Outputs:\n",
        "        tensor_input_batch_aug: dimensions: (100, 100, num_images X 40). \n",
        "        tensor_label_batch_aug: dimensions: (num_images X 40, 4)\n",
        "    '''\n",
        "\n",
        "    #Take batch of num_images rows from datafile:\n",
        "    with open(data_file, 'r') as f:\n",
        "        rows = f.readlines()[datafile_index:(datafile_index+num_images)]\n",
        "\n",
        "    #for batch size of 400 (augmented), need 10 images\n",
        "    data_batch = np.zeros((num_images,12101), dtype=np.dtype('U10'))\n",
        "    count = 0\n",
        "    for row in rows:\n",
        "        data_batch[count,:] = row.split()\n",
        "        count += 1\n",
        "\n",
        "    #separate label and input:\n",
        "    input_batch_flat = np.array(data_batch[:,:12100], dtype=float)#, dtype=int)\n",
        "    label_batch = np.array(data_batch[:,-1])\n",
        "\n",
        "    #convert input batch back to a 2D array:\n",
        "    input_batch = np.zeros((110,110,np.shape(input_batch_flat)[0]))#, dtype=int)\n",
        "    for ii in range(np.shape(input_batch_flat)[0]):\n",
        "        input_batch[:,:,ii] = np.reshape(input_batch_flat[ii,:], (110,110))\n",
        "\n",
        "\n",
        "    #convert label batch into into 1D vector: \n",
        "    #E=0, S0=1, Sp=2, Irr+Misc=3\n",
        "    #ex: label = [0,0,1,0] ==> Sp galagy\n",
        "    arr_label_batch = np.zeros((np.shape(label_batch)[0],4), dtype=int)\n",
        "    arr_label_batch[:,0] = np.array([label_batch == 'E'], dtype=int)\n",
        "    arr_label_batch[:,1] = np.array([label_batch == 'Sp'], dtype=int)\n",
        "    arr_label_batch[:,2] = np.array([label_batch == 'S0'], dtype=int)\n",
        "    arr_label_batch[:,3] = np.array([label_batch == 'Irr+Misc'], dtype=int)\n",
        "\n",
        "    if plotting == True:\n",
        "      #test with image plotted\n",
        "      plt.imshow(input_batch[:,:,0])\n",
        "      plt.show()\n",
        "\n",
        "    #NOW AUGMENT THE BATCH (40X more):\n",
        "    input_batch_aug = np.empty((100,100,np.shape(input_batch)[2]*40), dtype=int)\n",
        "    arr_label_batch_aug = np.empty((np.shape(arr_label_batch)[0]*40, 4), dtype=int)\n",
        "\n",
        "    count = 0\n",
        "    for ll in range(np.shape(input_batch)[2]):\n",
        "        #Crop 5X more image (100X100 pixels)\n",
        "        C1 = input_batch[:100,:100,ll]\n",
        "        C2 = input_batch[10:,:100,ll]\n",
        "        C3 = input_batch[:100,10:,ll]\n",
        "        C4 = input_batch[10:,10:,ll]\n",
        "        C5 = input_batch[5:105,5:105,ll]\n",
        "\n",
        "        C = [C1, C2, C3, C4, C5]\n",
        "\n",
        "        for kk in range(5):\n",
        "            #Rotate 4X more image (by 90 deg)\n",
        "            for jj in range(4):\n",
        "                C_R = np.rot90(C[kk], k=jj)\n",
        "                input_batch_aug[:,:,count] = C_R\n",
        "                arr_label_batch_aug[count,:] = arr_label_batch[ll,:]\n",
        "                count += 1\n",
        "                \n",
        "                input_batch_aug[:,:,count] = np.swapaxes(C_R,0,1)\n",
        "                arr_label_batch_aug[count,:] = arr_label_batch[ll,:]\n",
        "                count += 1\n",
        "\n",
        "\n",
        "    #PUT THE DATA AS A PYTORCH TENSOR:\n",
        "    tensor_input_batch_aug = torch.Tensor(input_batch_aug)\n",
        "    tensor_label_batch_aug = torch.Tensor(arr_label_batch_aug)\n",
        "    \n",
        "    return tensor_input_batch_aug, tensor_label_batch_aug\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "network_to_train = 'C1'\n",
        "\n",
        "# define hyperparameters of training\n",
        "if network_to_train == 'C1':\n",
        "  n_epochs = 12\n",
        "  cn_model = networkc1\n",
        "  optimizer = torch.optim.Adam(cn_model.parameters(), lr=2e-4)\n",
        "\n",
        "elif network_to_train == 'C2':\n",
        "  n_epochs = 20\n",
        "  cn_model = networkc2\n",
        "  lr = 2*pow(10,-4)\n",
        "  optimizer = torch.optim.Adam(cn_model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "5cu8-Z52PSNH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un1x2W4GAMex"
      },
      "source": [
        "# **Training**\n",
        "\n",
        "C2 uses Adam and C1 uses Adam: \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "OBjUVf7Xl0t9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize network & move to GPU\n",
        "cn_model.to('cuda')"
      ],
      "metadata": {
        "id": "F_HE_24Jl4MV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e36acfc4-c593-413b-a89f-c54bccfb79ff"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (1): ReLU()\n",
              "  (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (4): ReLU()\n",
              "  (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (7): Flatten(start_dim=1, end_dim=-1)\n",
              "  (8): Dropout(p=0.5, inplace=False)\n",
              "  (9): ReLU()\n",
              "  (10): Linear(in_features=135424, out_features=256, bias=True)\n",
              "  (11): ReLU()\n",
              "  (12): Linear(in_features=256, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define Train and test set\n",
        "dataset_size = 280\n",
        "train_split = 0.85# same as in paper\n",
        "test_split = 1 - train_split\n",
        "split_cutoff = int(dataset_size*train_split)\n",
        "\n",
        "rand_index = np.random.permutation(dataset_size)\n",
        "rand_train = rand_index[:split_cutoff] \n",
        "rand_test = rand_index[split_cutoff:dataset_size] # valudation will be taken from test set"
      ],
      "metadata": {
        "id": "qj1M9LgXI5_R"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For monitoring acc and losses\n",
        "avg_epoch_acc_train = []\n",
        "avg_epoch_acc_val = []\n",
        "avg_epoch_losses_train = []\n",
        "avg_epoch_losses_val = []\n",
        "\n",
        "num_images = 50 #number of images to augment in each batch\n",
        "batch_size = num_images*40 \n",
        "\n",
        "print('Model initialized and prepped, begin training...')\n",
        "\n",
        "for epoch in range(n_epochs):  \n",
        "    cn_model.train()\n",
        "    print('epoch:', epoch+1)\n",
        "\n",
        "    #VALIDATION FOR before any training, (used to check initialization)\n",
        "    if epoch == 0:\n",
        "        ds_valid_size = 0\n",
        "        cn_model.eval()\n",
        "        epoch_loss = 0\n",
        "        test_total_accuracy = 0\n",
        "        with torch.no_grad():\n",
        "          for ii in range(np.shape(rand_test)[0]):\n",
        "            im_valid, y_valid = data_batch(datafile_index=num_images*rand_test[ii], num_images=num_images)\n",
        "            im_valid = im_valid.reshape(100, 100, 1, im_valid.shape[2])\n",
        "            im_valid = im_valid.T\n",
        "\n",
        "            im_valid = im_valid.detach().to('cuda')\n",
        "            y_valid = y_valid.detach().to('cuda')\n",
        "\n",
        "            y_pred_valid = cn_model(im_valid)\n",
        "            y_pred_valid_cat = nn.functional.softmax(y_pred_valid, dim=1)\n",
        "\n",
        "            #updated accuracy calculation:\n",
        "            test_predictions = torch.argmax(y_pred_valid_cat, dim=1)\n",
        "            test_label_predictions = torch.argmax(y_valid, dim=1)\n",
        "            test_batch_size = np.shape(test_predictions)[0]\n",
        "            test_batch_accuracy = torch.sum(test_predictions == test_label_predictions).item()/test_batch_size\n",
        "            print(f'\\t\\t test batch accuracy = {np.round(100*test_batch_accuracy,2)} %, batch # {ds_valid_size}')\n",
        "            test_total_accuracy += test_batch_accuracy\n",
        "\n",
        "            loss = loss_fn(y_pred_valid, y_valid)\n",
        "            epoch_loss += loss.item()\n",
        "            ds_valid_size += 1\n",
        "\n",
        "            #delete image and label every loop train:\n",
        "            del im_valid\n",
        "            del y_valid\n",
        "            torch.cuda.empty_cache()\n",
        "          \n",
        "          #calculate total loss validation\n",
        "          v_loss = epoch_loss / ds_valid_size\n",
        "          avg_epoch_losses_val.append(v_loss)\n",
        "          print('validation loss:', np.round(v_loss,2))\n",
        "\n",
        "          #calculate total accuracy validation\n",
        "          test_total_accuracy = 100 * test_total_accuracy / np.shape(rand_test)[0]\n",
        "          print('Validation accuracy:', np.round(test_total_accuracy,2), '%')\n",
        "          avg_epoch_acc_val.append(test_total_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "    #get training dataset size\n",
        "    ds_size = 0\n",
        "    \n",
        "    train_total_accuracy = 0\n",
        "    epoch_loss = 0\n",
        "    for ii in range(np.shape(rand_train)[0]):\n",
        "      optimizer.zero_grad() #reset the gradients\n",
        "\n",
        "      im2, y = data_batch(datafile_index=num_images*rand_train[ii], num_images=num_images)\n",
        "\n",
        "      # reshaping im to what we want\n",
        "      im2 = im2.reshape(100, 100, 1, im2.shape[2])\n",
        "      im = im2.T\n",
        "\n",
        "      del im2\n",
        "\n",
        "      im = im.detach().to('cuda')\n",
        "      y = y.detach().to('cuda')\n",
        "\n",
        "      y_pred = cn_model(im)\n",
        "      y_pred_cat = nn.functional.softmax(y_pred, dim=1)\n",
        "      \n",
        "\n",
        "      #updated accuracy calculation:\n",
        "      train_predictions = torch.argmax(y_pred_cat, dim=1)\n",
        "      train_label_predictions = torch.argmax(y, dim=1)\n",
        "      train_batch_size = np.shape(train_predictions)[0]\n",
        "      train_batch_accuracy = torch.sum(train_predictions == train_label_predictions).item()/train_batch_size\n",
        "      print(f'\\t\\t train batch accuracy = {np.round(100*train_batch_accuracy,2)} %, batch # {ds_size}')\n",
        "      train_total_accuracy += train_batch_accuracy\n",
        "\n",
        "      #doing the backprop after each batch\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      epoch_loss += loss.item()\n",
        "      ds_size += 1\n",
        "\n",
        "      del im\n",
        "      del y\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    t_loss = epoch_loss / ds_size\n",
        "    print('training loss:', np.round(t_loss,2))\n",
        "    avg_epoch_losses_train.append(t_loss)\n",
        "\n",
        "    train_total_accuracy = 100 * train_total_accuracy / np.shape(rand_train)[0]\n",
        "    print('training accuracy:', np.round(train_total_accuracy,2), '%')\n",
        "    avg_epoch_acc_train.append(train_total_accuracy)\n",
        "\n",
        "\n",
        "    #Full VALIDATION:----------------------------------\n",
        "\n",
        "    ds_valid_size = 0\n",
        "    cn_model.eval() \n",
        "    epoch_loss = 0\n",
        "    test_total_accuracy = 0\n",
        "    with torch.no_grad():\n",
        "      for ii in range(np.shape(rand_test)[0]):\n",
        "        im_valid, y_valid = data_batch(datafile_index=num_images*rand_test[ii], num_images=num_images)\n",
        "        im_valid = im_valid.reshape(100, 100, 1, im_valid.shape[2])\n",
        "        im_valid = im_valid.T\n",
        "\n",
        "        im_valid = im_valid.detach().to('cuda')\n",
        "        y_valid = y_valid.detach().to('cuda')\n",
        "\n",
        "        y_pred_valid = cn_model(im_valid)\n",
        "        y_pred_valid_cat = nn.functional.softmax(y_pred_valid, dim=1)\n",
        "\n",
        "        #updated accuracy calculation:\n",
        "        test_predictions = torch.argmax(y_pred_valid_cat, dim=1)\n",
        "        test_label_predictions = torch.argmax(y_valid, dim=1)\n",
        "        test_batch_size = np.shape(test_predictions)[0]\n",
        "        test_batch_accuracy = torch.sum(test_predictions == test_label_predictions).item()/test_batch_size\n",
        "        print(f'\\t\\t test batch accuracy = {np.round(100*test_batch_accuracy,2)} %, batch # {ds_valid_size}')\n",
        "        test_total_accuracy += test_batch_accuracy\n",
        "\n",
        "        loss = loss_fn(y_pred_valid, y_valid)\n",
        "        epoch_loss += loss.item()\n",
        "        ds_valid_size += 1\n",
        "\n",
        "        #delete image and label every loop train:\n",
        "        del im_valid\n",
        "        del y_valid\n",
        "        torch.cuda.empty_cache()\n",
        "      \n",
        "      #calculate total loss validation\n",
        "      v_loss = epoch_loss / ds_valid_size\n",
        "      avg_epoch_losses_val.append(v_loss)\n",
        "      print('validation loss:', np.round(v_loss,2))\n",
        "\n",
        "      #calculate total accuracy validation\n",
        "      test_total_accuracy = 100 * test_total_accuracy / np.shape(rand_test)[0]\n",
        "      print('Validation accuracy:', np.round(test_total_accuracy,2), '%')\n",
        "      avg_epoch_acc_val.append(test_total_accuracy)\n",
        "\n",
        "\n",
        "print(\"DONE TRAINING\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "12RTbYBaK-wj",
        "outputId": "588a054e-e72e-42c5-d9e1-afed953bda90"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized and prepped, begin training...\n",
            "epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-503f3112862b>:26: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matricesor `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2981.)\n",
            "  im_valid = im_valid.T\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-503f3112862b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0my_pred_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0my_pred_valid_cat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    451\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 453\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    454\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cYok9ykjme8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ashley's troubleshooting notes:\n",
        "- I actually run into the issue again when I try num_images = 50, but num_images = 10 seems to be totally fine\n",
        "- I thought the issue was due to us using empty in the data loading function and it using old memory so I did make some changes in that function like replacing that with zeros\n",
        "- C1 might not be working due to different stride and no set learning rate\n",
        "- The paper also seems to have weird learning results..."
      ],
      "metadata": {
        "id": "SQ4FxiqUdOpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_acc = np.array(avg_epoch_acc_train)\n",
        "valid_acc = np.array(avg_epoch_acc_val)\n",
        "train_loss = np.array(avg_epoch_losses_train)\n",
        "valid_loss = np.array(avg_epoch_losses_val)\n",
        "\n",
        "print(train_acc, valid_acc, train_loss, valid_loss)\n",
        "\n",
        "#Plot accuracy results:\n",
        "plt.figure(figsize=(7,5)) #set plot size\n",
        "\n",
        "plt.plot(range(np.shape(train_acc)[0]), train_acc, label='Training Accuracy', \n",
        "             linestyle='-', color='red', linewidth=3)\n",
        "plt.plot(range(np.shape(valid_acc)[0]), valid_acc, label='Validation Accuracy', \n",
        "             linestyle='-', color='blue', linewidth=3)\n",
        "\n",
        "plt.yticks(fontsize=12, rotation=0) #adjust axis tick numbers font size\n",
        "plt.xticks(fontsize=12, rotation=0) #adjust axis tick numbers font size\n",
        "plt.xlabel('Epoch Number', fontsize=14) #set axis label\n",
        "plt.ylabel('Percent Accuracy', fontsize=14) #set axis label\n",
        "plt.title('Training of 4 way C2 CNN Network', fontsize=16) #set title\n",
        "plt.legend(fontsize=10)\n",
        "plt.xlim(0, np.shape(train_acc)[0]-1) #set axis limits\n",
        "plt.grid(True, which='minor', color='gray', linestyle='--', linewidth=1, alpha=0.2) #set gridlines\n",
        "plt.grid(True, which='major', color='gray', linestyle='-', linewidth=1, alpha=0.5) #set gridlines\n",
        "plt.tight_layout()\n",
        "plt.savefig(CWD+'data/Jordan_C1_4way_Accuracy_plot_v1.png',dpi=300)\n",
        "#plt.close() #Stops the figure from being shown\n",
        "plt.show() #display the figure\n",
        "\n",
        "\n",
        "\n",
        "#Plot loss results:\n",
        "plt.figure(figsize=(7,5)) #set plot size\n",
        "\n",
        "plt.plot(range(np.shape(train_loss)[0]), train_loss, label='Training Loss', \n",
        "             linestyle='-', color='red', linewidth=3)\n",
        "plt.plot(range(np.shape(valid_loss)[0]), valid_loss, label='Validation Loss', \n",
        "             linestyle='-', color='blue', linewidth=3)\n",
        "\n",
        "plt.yticks(fontsize=12, rotation=0) #adjust axis tick numbers font size\n",
        "plt.xticks(fontsize=12, rotation=0) #adjust axis tick numbers font size\n",
        "plt.xlabel('Epoch Number', fontsize=14) #set axis label\n",
        "plt.ylabel('Loss', fontsize=14) #set axis label\n",
        "plt.title('Training of 4 way C2 CNN Network', fontsize=16) #set title\n",
        "plt.legend(fontsize=10)\n",
        "plt.xlim(0, np.shape(train_acc)[0]-1) #set axis limits\n",
        "plt.grid(True, which='minor', color='gray', linestyle='--', linewidth=1, alpha=0.2) #set gridlines\n",
        "plt.grid(True, which='major', color='gray', linestyle='-', linewidth=1, alpha=0.5) #set gridlines\n",
        "plt.tight_layout()\n",
        "#plt.yscale('log')\n",
        "plt.savefig(CWD+'data/Jordan_C1_4way_Loss_plot_v1.png',dpi=300)\n",
        "#plt.close() #Stops the figure from being shown\n",
        "plt.show() #display the figure"
      ],
      "metadata": {
        "id": "O271JYPaVgGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save model itself \n",
        "torch.save(cn_model.state_dict(), CWD+'data/C1_4way_Full_Augmentation_model')#, CWD + 'Notebooks/models/')"
      ],
      "metadata": {
        "id": "4-K30VyfjWYU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}