{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashley-ferreira/PHYS449_FinalProject/blob/main/notebooks/KerasC2_our_data_5x.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z41xLe93yrK8",
        "outputId": "599d251d-208f-4ae7-ea55-58d4ae76b0ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting astroNN\n",
            "  Downloading astroNN-1.0.1.tar.gz (9.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.3 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from astroNN) (1.21.6)\n",
            "Requirement already satisfied: astropy in /usr/local/lib/python3.8/dist-packages (from astroNN) (4.3.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from astroNN) (3.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from astroNN) (3.2.2)\n",
            "Collecting astroquery\n",
            "  Downloading astroquery-0.4.6-py3-none-any.whl (4.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5 MB 81.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from astroNN) (1.3.5)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.8/dist-packages (from astroNN) (0.11.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from astroNN) (1.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from astroNN) (4.64.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from astroNN) (21.3)\n",
            "Requirement already satisfied: pyerfa>=1.7.3 in /usr/local/lib/python3.8/dist-packages (from astropy->astroNN) (2.0.0.1)\n",
            "Requirement already satisfied: html5lib>=0.999 in /usr/local/lib/python3.8/dist-packages (from astroquery->astroNN) (1.0.1)\n",
            "Collecting keyring>=4.0\n",
            "  Downloading keyring-23.11.0-py3-none-any.whl (36 kB)\n",
            "Collecting pyvo>=1.1\n",
            "  Downloading pyvo-1.4-py3-none-any.whl (885 kB)\n",
            "\u001b[K     |████████████████████████████████| 885 kB 60.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from astroquery->astroNN) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.3.2 in /usr/local/lib/python3.8/dist-packages (from astroquery->astroNN) (4.6.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from html5lib>=0.999->astroquery->astroNN) (1.15.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from html5lib>=0.999->astroquery->astroNN) (0.5.1)\n",
            "Collecting jaraco.classes\n",
            "  Downloading jaraco.classes-3.2.3-py3-none-any.whl (6.0 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.11.4 in /usr/local/lib/python3.8/dist-packages (from keyring>=4.0->astroquery->astroNN) (4.13.0)\n",
            "Collecting jeepney>=0.4.2\n",
            "  Downloading jeepney-0.8.0-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting SecretStorage>=3.2\n",
            "  Downloading SecretStorage-3.3.3-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.11.4->keyring>=4.0->astroquery->astroNN) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.4.3->astroquery->astroNN) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.4.3->astroquery->astroNN) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.4.3->astroquery->astroNN) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.4.3->astroquery->astroNN) (2022.9.24)\n",
            "Collecting cryptography>=2.0\n",
            "  Downloading cryptography-38.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 70.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.8/dist-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring>=4.0->astroquery->astroNN) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring>=4.0->astroquery->astroNN) (2.21)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from jaraco.classes->keyring>=4.0->astroquery->astroNN) (9.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->astroNN) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->astroNN) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->astroNN) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->astroNN) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->astroNN) (2022.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->astroNN) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->astroNN) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->astroNN) (3.1.0)\n",
            "Building wheels for collected packages: astroNN\n",
            "  Building wheel for astroNN (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for astroNN: filename=astroNN-1.0.1-py3-none-any.whl size=9284593 sha256=a5f4db112afdd8b2b01c0c1c97b316543fb08f9331797c10f1ccc7dc978f6897\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/b6/1f/222aea123a5de8a34c3dd95bb73dca35e342ef3901328e9db0\n",
            "Successfully built astroNN\n",
            "Installing collected packages: jeepney, cryptography, SecretStorage, jaraco.classes, pyvo, keyring, astroquery, astroNN\n",
            "Successfully installed SecretStorage-3.3.3 astroNN-1.0.1 astroquery-0.4.6 cryptography-38.0.4 jaraco.classes-3.2.3 jeepney-0.8.0 keyring-23.11.0 pyvo-1.4\n"
          ]
        }
      ],
      "source": [
        "!pip install astroNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yMvvMay_bgma"
      },
      "outputs": [],
      "source": [
        "#from astroNN.datasets import galaxy10\n",
        "#from astroNN.datasets.galaxy10 import galaxy10cls_lookup\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0L6m3MIYz_p",
        "outputId": "8cbbafe1-a2d1-4306-b778-7dacb60f3cb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Import google drive (need to put data folder as shortcut in your local drive My Drive):\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQRml3VHevfV"
      },
      "source": [
        "# **Full data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dh5a6h6wY-UM"
      },
      "outputs": [],
      "source": [
        "# #LOAD THE DATA FROM TXT FILE INTO A BATCH:\n",
        "# def data_batch(datafile_index, num_images=40, data_file='MyDrive/data_g_band.txt', plotting=False):\n",
        "#     '''\n",
        "#     Description:\n",
        "#         Access datafile.txt, each row is flattened 110x110 image + 1 label string (E, Sp, S0, Irr+Misc).\n",
        "#         Returns an augmented batch of num_images X 40.\n",
        "#         The labels are converted to 1D vectors (ex: Sp = [0,0,1,0])\n",
        "#         Need to give a datafile_index that tells which rows to pick.\n",
        "#     Inputs:\n",
        "#         datafile_index: index of row in datafile to load. loads rows datafile_index to datafile_index+num_images.\n",
        "#         num_images: number of different images to load per batch, total batch size \n",
        "#         is 40 X num_images. (default: 10 (for 40X10 = 400 batch size like in paper)\n",
        "#         data_file: datafile full path, need to add shortcut to local Drive. (default: '/content/drive/MyDrive/data/data_g_band.txt')\n",
        "#     Outputs:\n",
        "#         tensor_input_batch_aug: dimensions: (100, 100, num_images X 40).\n",
        "#         tensor_label_batch_aug: dimensions: (num_images X 40, 4)\n",
        "#     '''\n",
        "\n",
        "#     #Take batch of num_images rows from datafile:\n",
        "#     with open(data_file, 'r') as f:\n",
        "#         rows = f.readlines()[datafile_index:(datafile_index+num_images)]\n",
        "\n",
        "#     #for batch size of 400 (augmented), need 10 images\n",
        "#     data_batch = np.zeros((num_images,12101), dtype=np.dtype('U10'))\n",
        "#     count = 0\n",
        "#     for row in rows:\n",
        "#         data_batch[count,:] = row.split()\n",
        "#         count += 1\n",
        "\n",
        "#     #separate label and input:\n",
        "#     input_batch_flat = np.array(data_batch[:,:12100], dtype=float)#, dtype=int)\n",
        "#     label_batch = np.array(data_batch[:,-1])\n",
        "\n",
        "#     #convert input batch back to a 2D array:\n",
        "#     input_batch = np.zeros((110,110,np.shape(input_batch_flat)[0]))#, dtype=int)\n",
        "#     for ii in range(np.shape(input_batch_flat)[0]):\n",
        "#         input_batch[:,:,ii] = np.reshape(input_batch_flat[ii,:], (110,110))\n",
        "\n",
        "\n",
        "#     #convert label batch into into 1D vector: \n",
        "#     #E=0, S0=1, Sp=2, Irr+Misc=3\n",
        "#     #ex: label = [0,0,1,0] ==> Sp galagy\n",
        "#     arr_label_batch = np.zeros((np.shape(label_batch)[0],4), dtype=int)\n",
        "#     arr_label_batch[:,0] = np.array([label_batch == 'E'], dtype=int)\n",
        "#     arr_label_batch[:,1] = np.array([label_batch == 'Sp'], dtype=int)\n",
        "#     arr_label_batch[:,2] = np.array([label_batch == 'S0'], dtype=int)\n",
        "#     arr_label_batch[:,3] = np.array([label_batch == 'Irr+Misc'], dtype=int)\n",
        "\n",
        "#     if plotting == True:\n",
        "#       #test with image plotted\n",
        "#       plt.imshow(input_batch[:,:,0])\n",
        "#       plt.show()\n",
        "\n",
        "#     #NOW AUGMENT THE BATCH (40X more):\n",
        "#     input_batch_aug = np.empty((100,100,np.shape(input_batch)[2]*40), dtype=int)\n",
        "#     arr_label_batch_aug = np.empty((np.shape(arr_label_batch)[0]*40, 4), dtype=int)\n",
        "\n",
        "#     count = 0\n",
        "#     for ll in range(np.shape(input_batch)[2]):\n",
        "#         #Crop 5X more image (100X100 pixels)\n",
        "#         C1 = input_batch[:100,:100,ll]\n",
        "#         C2 = input_batch[10:,:100,ll]\n",
        "#         C3 = input_batch[:100,10:,ll]\n",
        "#         C4 = input_batch[10:,10:,ll]\n",
        "#         C5 = input_batch[5:105,5:105,ll]\n",
        "\n",
        "#         C = [C1, C2, C3, C4, C5]\n",
        "\n",
        "#         for kk in range(5):\n",
        "#             #Rotate 4X more image (by 90 deg)\n",
        "#             for jj in range(4):\n",
        "#                 C_R = np.rot90(C[kk], k=jj)\n",
        "#                 input_batch_aug[:,:,count] = C_R\n",
        "#                 arr_label_batch_aug[count,:] = arr_label_batch[ll,:]\n",
        "#                 count += 1\n",
        "                \n",
        "#                 input_batch_aug[:,:,count] = np.swapaxes(C_R,0,1)\n",
        "#                 arr_label_batch_aug[count,:] = arr_label_batch[ll,:]\n",
        "#                 count += 1\n",
        "\n",
        "\n",
        "#     #PUT THE DATA AS A PYTORCH TENSOR:\n",
        "#     #tensor_input_batch_aug = torch.Tensor(input_batch_aug)\n",
        "#     #tensor_label_batch_aug = torch.Tensor(arr_label_batch_aug)\n",
        "    \n",
        "#     return input_batch_aug, arr_label_batch_aug"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ilvG8-Ie4my"
      },
      "source": [
        "# **Our augmented data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9czaV48Ge4eq"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Z-BooKMxZIt0",
        "outputId": "9dbc0c88-a2eb-47b9-b5a9-4fd5b36baf87"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#Test above function:\\nrand_index = np.random.permutation(1403) #10 images\\nrand_train = rand_index[:200] #arbitrary values\\nrand_test = rand_index[200:]\\n\\n#Use this loop for training over entire dataset at each epochs\\nfor ii in range(np.shape(rand_train)[0]):\\n  image_batch, label_batch = data_batch_aug5(datafile_index=10*rand_train[ii], num_images=10)\\n  ##print(np.shape(image_batch))\\n  ##print(np.shape(label_batch))\\n  ##print(label_batch)\\n  #Check: 10 images X 5 augmentation = 100 x 100 x 50 tensor size\\n  #check: label size is 10 x 5 = 50 x 4 (4 labels)\\n  #check: label is 5 type in a row then another 5 in a row etc ...\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#LESS DATA AUGMENTION: crop only = X5 augmentation:\n",
        "\n",
        "#AUGMENT ONLY X5 (ONLY BY CROPPING)\n",
        "def data_batch_aug5(datafile_index, num_images=10,  data_file='/content/drive/MyDrive/data_g_band.txt', plotting=False):\n",
        "    '''\n",
        "    Description:\n",
        "        Access datafile.txt, each row is flattened 110x110 image + 1 label string (E, Sp, S0, Irr+Misc).\n",
        "        Returns an augmented batch of num_images X 5.\n",
        "        The labels are converted to 1D vectors (ex: Sp = [0,0,1,0])\n",
        "        Need to give a datafile_index that tells which rows to pick.\n",
        "    Inputs:\n",
        "        datafile_index: index of row in datafile to load. loads rows datafile_index to datafile_index+num_images.\n",
        "        num_images: number of different images to load per batch, total batch size \n",
        "        is 5 X num_images. (default: 10 (for 5X10 = 400 batch size like in paper)\n",
        "        data_file: datafile full path, need to add shortcut to local Drive. (default: '/content/drive/MyDrive/data/data_g_band.txt')\n",
        "    Outputs:\n",
        "        tensor_input_batch_aug: dimensions: (100, 100, num_images X 5). \n",
        "        tensor_label_batch_aug: dimensions: (num_images X 5, 4)\n",
        "    '''\n",
        "\n",
        "    #data_file = 'data_g_band.txt'\n",
        "\n",
        "    #Take batch of num_images rows from datafile:\n",
        "    with open(data_file, 'r') as f:\n",
        "        rows = f.readlines()[datafile_index:(datafile_index+num_images)]\n",
        "\n",
        "    #for batch size of 400 (augmented), need 10 images\n",
        "    data_batch = np.zeros((num_images,12101), dtype=np.dtype('U10'))\n",
        "    count = 0\n",
        "    for row in rows:\n",
        "        data_batch[count,:] = row.split()\n",
        "        count += 1\n",
        "\n",
        "    #separate label and input:\n",
        "    input_batch_flat = np.array(data_batch[:,:12100], dtype=int)\n",
        "    label_batch = np.array(data_batch[:,-1])\n",
        "\n",
        "    #convert input batch back to a 2D array:\n",
        "    input_batch = np.empty((110,110,np.shape(input_batch_flat)[0]), dtype=int)\n",
        "    for ii in range(np.shape(input_batch_flat)[0]):\n",
        "        input_batch[:,:,ii] = np.reshape(input_batch_flat[ii,:], (110,110))\n",
        "\n",
        "\n",
        "    #convert label batch into into 1D vector: \n",
        "    #E=0, S0=1, Sp=2, Irr+Misc=3\n",
        "    #ex: label = [0,0,1,0] ==> Sp galagy\n",
        "    arr_label_batch = np.empty((np.shape(label_batch)[0],4), dtype=int)\n",
        "    arr_label_batch[:,0] = np.array([label_batch == 'E'], dtype=int)\n",
        "    arr_label_batch[:,1] = np.array([label_batch == 'Sp'], dtype=int)\n",
        "    arr_label_batch[:,2] = np.array([label_batch == 'S0'], dtype=int)\n",
        "    arr_label_batch[:,3] = np.array([label_batch == 'Irr+Misc'], dtype=int)\n",
        "\n",
        "    #test with image plotted\n",
        "    if plotting == True:\n",
        "      plt.imshow(input_batch[:,:,0])\n",
        "      plt.show()\n",
        "\n",
        "    #NOW AUGMENT THE BATCH (5X more):\n",
        "    how_much_augment = 5\n",
        "    input_batch_aug = np.empty((100,100,np.shape(input_batch)[2]*how_much_augment), dtype=int)\n",
        "    arr_label_batch_aug = np.empty((np.shape(arr_label_batch)[0]*how_much_augment, 4), dtype=int)\n",
        "\n",
        "    count = 0\n",
        "    for ll in range(np.shape(input_batch)[2]):\n",
        "        #Crop 5X more image (100X100 pixels)\n",
        "        C1 = input_batch[:100,:100,ll]\n",
        "        C2 = input_batch[10:,:100,ll]\n",
        "        C3 = input_batch[:100,10:,ll]\n",
        "        C4 = input_batch[10:,10:,ll]\n",
        "        C5 = input_batch[5:105,5:105,ll]\n",
        "\n",
        "        C = [C1, C2, C3, C4, C5]\n",
        "\n",
        "        for kk in range(5):\n",
        "            input_batch_aug[:,:,count] = C[kk]\n",
        "            arr_label_batch_aug[count,:] = arr_label_batch[ll,:]\n",
        "            count += 1\n",
        "\n",
        "    #PUT THE DATA AS A PYTORCH TENSOR:\n",
        "    #tensor_input_batch_aug = torch.Tensor(input_batch_aug)\n",
        "    #tensor_label_batch_aug = torch.Tensor(arr_label_batch_aug)\n",
        "    \n",
        "    return input_batch_aug, arr_label_batch_aug\n",
        "\n",
        "'''\n",
        "#Test above function:\n",
        "rand_index = np.random.permutation(1403) #10 images\n",
        "rand_train = rand_index[:200] #arbitrary values\n",
        "rand_test = rand_index[200:]\n",
        "\n",
        "#Use this loop for training over entire dataset at each epochs\n",
        "for ii in range(np.shape(rand_train)[0]):\n",
        "  image_batch, label_batch = data_batch_aug5(datafile_index=10*rand_train[ii], num_images=10)\n",
        "  ##print(np.shape(image_batch))\n",
        "  ##print(np.shape(label_batch))\n",
        "  ##print(label_batch)\n",
        "  #Check: 10 images X 5 augmentation = 100 x 100 x 50 tensor size\n",
        "  #check: label size is 10 x 5 = 50 x 4 (4 labels)\n",
        "  #check: label is 5 type in a row then another 5 in a row etc ...\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbjMNWI2v13Z"
      },
      "source": [
        "# **Get data for a select index**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc9NLJ2EfKW7",
        "outputId": "8c255c3f-f1c3-4338-a23d-1763a1d707c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 100, 70000)\n"
          ]
        }
      ],
      "source": [
        "input_batch_aug, arr_label_batch_aug = data_batch_aug5(datafile_index=1, num_images=14000,  data_file='/content/drive/MyDrive/data_g_band.txt', plotting=False)\n",
        "print(input_batch_aug.shape)\n",
        "len_input = len(input_batch_aug.T)\n",
        "#input_batch_aug.reshape((1, 100, 100, 9000))\n",
        "#print(input_batch_aug.shape)\n",
        "#print(arr_label_batch_aug.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHlAUKTmTZiq",
        "outputId": "52c8b38c-abdf-4fe7-c614-bf24b2c994f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 100, 100, 70000)\n"
          ]
        }
      ],
      "source": [
        "#print(input_batch_aug.shape)\n",
        "input_batch_aug_final = input_batch_aug.reshape(1, 100, 100, len_input)\n",
        "print(input_batch_aug_final.shape)\n",
        "#print(arr_label_batch_aug.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqN3pmAk0G-7"
      },
      "source": [
        "# **Galaxy10 (not in use)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qmejelzhyfBe"
      },
      "outputs": [],
      "source": [
        "# # ---------------------------------------------------------#\n",
        "# #   astroNN.datasets.galaxy10: galaxy10\n",
        "# # ---------------------------------------------------------#\n",
        "\n",
        "# import os\n",
        "# import urllib.request\n",
        "\n",
        "# import h5py\n",
        "# import numpy as np\n",
        "\n",
        "# from astroNN.config import astroNN_CACHE_DIR\n",
        "# from astroNN.shared.downloader_tools import TqdmUpTo\n",
        "# from astroNN.shared.downloader_tools import sha256_checksum\n",
        "\n",
        "# Galaxy10Class = {0: \"Disk, Face-on, No Spiral\",\n",
        "#                  1: \"Smooth, Completely round\",\n",
        "#                  2: \"Smooth, in-between round\",\n",
        "#                  3: \"Smooth, Cigar shaped\",\n",
        "#                  4: \"Disk, Edge-on, Rounded Bulge\",\n",
        "#                  5: \"Disk, Edge-on, Boxy Bulge\",\n",
        "#                  6: \"Disk, Edge-on, No Bulge\",\n",
        "#                  7: \"Disk, Face-on, Tight Spiral\",\n",
        "#                  8: \"Disk, Face-on, Medium Spiral\",\n",
        "#                  9: \"Disk, Face-on, Loose Spiral\"}\n",
        "\n",
        "# _G10_ORIGIN = 'http://astro.utoronto.ca/~bovy/Galaxy10/'\n",
        "\n",
        "\n",
        "# def load_data(flag=None):\n",
        "#     \"\"\"\n",
        "#     NAME:\n",
        "#         load_data\n",
        "#     PURPOSE:\n",
        "#         load_data galaxy10 data\n",
        "#     INPUT:\n",
        "#         None\n",
        "#     OUTPUT:\n",
        "#         x (ndarray): An array of images\n",
        "#         y (ndarray): An array of answer\n",
        "#     HISTORY:\n",
        "#         2018-Jan-22 - Written - Henry Leung (University of Toronto)\n",
        "#     \"\"\"\n",
        "\n",
        "#     filename = 'Galaxy10.h5'\n",
        "\n",
        "#     complete_url = _G10_ORIGIN + filename\n",
        "\n",
        "#     datadir = os.path.join(astroNN_CACHE_DIR, 'datasets')\n",
        "#     file_hash = '969A6B1CEFCC36E09FFFA86FEBD2F699A4AA19B837BA0427F01B0BC6DED458AF'  # SHA256\n",
        "\n",
        "#     # Notice python expect sha256 in lowercase\n",
        "\n",
        "#     if not os.path.exists(datadir):\n",
        "#         os.makedirs(datadir)\n",
        "#     fullfilename = os.path.join(datadir, filename)\n",
        "\n",
        "#     # Check if files exists\n",
        "#     if os.path.isfile(fullfilename) and flag is None:\n",
        "#         checksum = sha256_checksum(fullfilename)\n",
        "#         if checksum != file_hash.lower():\n",
        "#             print('File corruption detected, astroNN attempting to download again')\n",
        "#             load_data(flag=1)\n",
        "#         else:\n",
        "#             print(fullfilename + ' was found!')\n",
        "#     elif not os.path.isfile(fullfilename) or flag == 1:\n",
        "#         with TqdmUpTo(unit='B', unit_scale=True, miniters=1, desc=complete_url.split('/')[-1]) as t:\n",
        "#             urllib.request.urlretrieve(complete_url, fullfilename, reporthook=t.update_to)\n",
        "#             print(f'Downloaded Galaxy10 successfully to {fullfilename}')\n",
        "#             checksum = sha256_checksum(fullfilename)\n",
        "#             if checksum != file_hash.lower():\n",
        "#                 load_data(flag=1)\n",
        "\n",
        "#     with h5py.File(fullfilename, 'r') as F:\n",
        "#         x = np.array(F['images'])\n",
        "#         y = np.array(F['ans'])\n",
        "\n",
        "#     return x, y\n",
        "\n",
        "\n",
        "# def galaxy10cls_lookup(class_num):\n",
        "#     \"\"\"\n",
        "#     NAME:\n",
        "#         galaxy10cls_lookup\n",
        "#     PURPOSE:\n",
        "#         look up class name for Galaxy10\n",
        "#     INPUT:\n",
        "#         class_num (int): An integer 0-9\n",
        "#     OUTPUT:\n",
        "#         (string): Name of the class\n",
        "#     HISTORY:\n",
        "#         2018-Feb-07 - Written - Henry Leung (University of Toronto)\n",
        "#     \"\"\"\n",
        "#     if isinstance(class_num, list) or isinstance(class_num, np.ndarray):\n",
        "#         class_num = np.argmax(class_num)\n",
        "#     if 0 > class_num or 9 < class_num:\n",
        "#         raise ValueError(f'Galaxy10 only has 10 classes, you entered {class_num}')\n",
        "#     return Galaxy10Class[class_num]\n",
        "\n",
        "\n",
        "# def galaxy10_confusion(confusion_mat):\n",
        "#     \"\"\"\n",
        "#     NAME:\n",
        "#         galaxy10_confusion\n",
        "#     PURPOSE:\n",
        "#         to plot confusion matrix\n",
        "#     INPUT:\n",
        "#         confusion_mat (ndarray): An integer 0-9\n",
        "#     OUTPUT:\n",
        "#         (string): Name of the class\n",
        "#     HISTORY:\n",
        "#         2018-Feb-11 - Written - Henry Leung (University of Toronto)\n",
        "#     \"\"\"\n",
        "#     import pylab as plt\n",
        "\n",
        "#     conf_arr = confusion_mat.astype(int)\n",
        "\n",
        "#     norm_conf = []\n",
        "#     a = np.max(conf_arr)\n",
        "#     for i in conf_arr:\n",
        "#         tmp_arr = []\n",
        "#         for j in i:\n",
        "#             tmp_arr.append(float(j) / float(a))\n",
        "#         norm_conf.append(tmp_arr)\n",
        "\n",
        "#     fig, ax = plt.subplots(1, figsize=(10, 10.5), dpi=100)\n",
        "#     fig.suptitle(\"Confusion Matrix for Galaxy10 trained by astroNN\", fontsize=18)\n",
        "#     ax.set_aspect(1)\n",
        "#     ax.imshow(np.array(norm_conf), cmap=plt.get_cmap('Blues'), interpolation='nearest')\n",
        "\n",
        "#     width, height = conf_arr.shape\n",
        "\n",
        "#     for x in range(width):\n",
        "#         for y in range(height):\n",
        "#             ax.annotate(str(conf_arr[x][y]), xy=(y, x),\n",
        "#                         horizontalalignment='center',\n",
        "#                         verticalalignment='center')\n",
        "\n",
        "#     alphabet = '0123456789'\n",
        "#     plt.xticks(range(width), alphabet[:width], fontsize=20)\n",
        "#     plt.yticks(range(height), alphabet[:height], fontsize=20)\n",
        "#     ax.set_ylabel('Prediction class by astroNN', fontsize=18)\n",
        "#     ax.set_xlabel('True class', fontsize=18)\n",
        "#     fig.tight_layout(rect=[0, 0.00, 0.8, 0.96])\n",
        "#     fig.show()\n",
        "\n",
        "#     return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpfdzqWCy7jv"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "myhOnV-Xy7uZ"
      },
      "outputs": [],
      "source": [
        "# images, labels = load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZnssA2Wy-d1"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5dOYsWKCy-nO"
      },
      "outputs": [],
      "source": [
        "# print('l shape initial =', labels.shape)\n",
        "# print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6_W3yWM6zGAo"
      },
      "outputs": [],
      "source": [
        "# labels = labels.astype(np.float32)\n",
        "# print('l shape before =', labels.shape)\n",
        "# labels = tf.keras.utils.to_categorical(labels)\n",
        "# print('l shape after =', labels.shape)\n",
        "# images = images.astype(np.float32)\n",
        "# print('i shape before =', images.shape)\n",
        "# images = images/255\n",
        "# print('i shape after =', images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD2hiNVNzQbx"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YEqJLYcTzQkV"
      },
      "outputs": [],
      "source": [
        "# Sample images\n",
        "# def show_image(image_num, label_num):\n",
        "#     label_name = int(np.where(label_num==1)[0])\n",
        "#     label = galaxy10cls_lookup(label_name)\n",
        "#     plt.imshow(image_num)\n",
        "#     plt.title(label)\n",
        "#     plt.show()\n",
        "# for i in range(0,10):\n",
        "#     show_image(images[i], labels[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rBsl82yzRWD"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_MzWyoKHzRbo"
      },
      "outputs": [],
      "source": [
        "# #This takes the old labels with 10 classes and turns it into 4\n",
        "# elliptical = [1,2,3]\n",
        "# lenticular = [4,5,6]\n",
        "# spiral = [7,8,9]\n",
        "# irregular = [0]\n",
        "\n",
        "# def change_class4(lable):\n",
        "#     label_num_pos = int(np.where(lable==1)[0])\n",
        "#     if label_num_pos in elliptical:\n",
        "#         new_name = np.array([1.,0.,0.,0.])\n",
        "#     if label_num_pos in lenticular:\n",
        "#         new_name = np.array([0.,1.,0.,0.])\n",
        "#     if label_num_pos in spiral:\n",
        "#         new_name = np.array([0.,0.,1.,0.])\n",
        "#     if label_num_pos in irregular:\n",
        "#         new_name = np.array([0.,0.,0.,1.])\n",
        "#     return new_name\n",
        "# new_labels4 = np.array([change_class4(labels[i]) for i in range(len(labels))])\n",
        "# print(new_labels4[:10])\n",
        "# print(labels.shape, new_labels4.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCwdAhF3zUV_"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dyp91epKzUeE"
      },
      "outputs": [],
      "source": [
        "# #This shows the amount in each label\n",
        "# label_categor_nums = [int(np.where(new_labels4[i]==1)[0]) for i in range(len(new_labels4))]\n",
        "# #print(label_categor_nums[:15])\n",
        "# name4 = [0,1,2,3]\n",
        "# name_occur = [label_categor_nums.count(name4[i]) for i in range(len(name4))]\n",
        "# print(name_occur)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gXSPJ_czWlz"
      },
      "source": [
        "# **Data split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eget-kbzzWrm",
        "outputId": "9fa1117b-d4c3-48e7-e4be-1ff0b9f9cca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(59500, 100, 100, 1)\n",
            "(59500, 4)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(input_batch_aug_final.T, arr_label_batch_aug, test_size = 0.15)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlivDtAJzayP"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE7j0xKEza4E",
        "outputId": "6cad3e31-d2e0-4bf5-c2a5-53a7953ef54c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 94, 94, 32)        1600      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 94, 94, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 47, 47, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 43, 43, 64)        51264     \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 39, 39, 64)        102464    \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 39, 39, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 19, 19, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 17, 17, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 17, 17, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 8, 8, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 8192)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8192)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               2097408   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 4)                 1028      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,394,308\n",
            "Trainable params: 2,393,860\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(7, 7), input_shape=(100,100,1), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
        "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjNjh8lC1NzY",
        "outputId": "84e728eb-3db4-4dd9-d48c-2b438e4e9ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1860/1860 [==============================] - 26s 9ms/step - loss: 1.1081 - accuracy: 0.5384 - val_loss: 1.3524 - val_accuracy: 0.5498\n",
            "Epoch 2/20\n",
            "1860/1860 [==============================] - 15s 8ms/step - loss: 1.0695 - accuracy: 0.5472 - val_loss: 1.1400 - val_accuracy: 0.5401\n",
            "Epoch 3/20\n",
            "1860/1860 [==============================] - 15s 8ms/step - loss: 1.0560 - accuracy: 0.5493 - val_loss: 1.0426 - val_accuracy: 0.5524\n",
            "Epoch 4/20\n",
            "1860/1860 [==============================] - 15s 8ms/step - loss: 1.0361 - accuracy: 0.5620 - val_loss: 1.0296 - val_accuracy: 0.5658\n",
            "Epoch 5/20\n",
            "1860/1860 [==============================] - 15s 8ms/step - loss: 1.0071 - accuracy: 0.5776 - val_loss: 1.0096 - val_accuracy: 0.5877\n",
            "Epoch 6/20\n",
            "1860/1860 [==============================] - 15s 8ms/step - loss: 0.9832 - accuracy: 0.5893 - val_loss: 1.0575 - val_accuracy: 0.5769\n",
            "Epoch 7/20\n",
            "1860/1860 [==============================] - 15s 8ms/step - loss: 0.9679 - accuracy: 0.5962 - val_loss: 0.9960 - val_accuracy: 0.5936\n",
            "Epoch 8/20\n",
            "1860/1860 [==============================] - 15s 8ms/step - loss: 0.9513 - accuracy: 0.6036 - val_loss: 1.0303 - val_accuracy: 0.5829\n",
            "Epoch 9/20\n",
            "1860/1860 [==============================] - 15s 8ms/step - loss: 0.9450 - accuracy: 0.6080 - val_loss: 1.0065 - val_accuracy: 0.5837\n",
            "Epoch 10/20\n",
            "1860/1860 [==============================] - 15s 8ms/step - loss: 0.9338 - accuracy: 0.6117 - val_loss: 1.0166 - val_accuracy: 0.5836\n",
            "Epoch 11/20\n",
            "1860/1860 [==============================] - 15s 8ms/step - loss: 0.9238 - accuracy: 0.6152 - val_loss: 1.0257 - val_accuracy: 0.5709\n",
            "Epoch 12/20\n",
            "1860/1860 [==============================] - 15s 8ms/step - loss: 0.9154 - accuracy: 0.6195 - val_loss: 1.3492 - val_accuracy: 0.5456\n",
            "Epoch 13/20\n",
            "1860/1860 [==============================] - 15s 8ms/step - loss: 0.9036 - accuracy: 0.6253 - val_loss: 0.9515 - val_accuracy: 0.6176\n",
            "Epoch 14/20\n",
            "1860/1860 [==============================] - 15s 8ms/step - loss: 0.8941 - accuracy: 0.6283 - val_loss: 1.1316 - val_accuracy: 0.5838\n",
            "Epoch 15/20\n",
            "1860/1860 [==============================] - 15s 8ms/step - loss: 0.8798 - accuracy: 0.6335 - val_loss: 0.9319 - val_accuracy: 0.6172\n",
            "Epoch 16/20\n",
            "1860/1860 [==============================] - 15s 8ms/step - loss: 0.8699 - accuracy: 0.6378 - val_loss: 1.1971 - val_accuracy: 0.5883\n",
            "Epoch 17/20\n",
            "1860/1860 [==============================] - 15s 8ms/step - loss: 0.8576 - accuracy: 0.6435 - val_loss: 0.9339 - val_accuracy: 0.6248\n",
            "Epoch 18/20\n",
            "1860/1860 [==============================] - 15s 8ms/step - loss: 0.8464 - accuracy: 0.6484 - val_loss: 0.9367 - val_accuracy: 0.6274\n",
            "Epoch 19/20\n",
            "1860/1860 [==============================] - 15s 8ms/step - loss: 0.8320 - accuracy: 0.6557 - val_loss: 0.8970 - val_accuracy: 0.6326\n",
            "Epoch 20/20\n",
            "1860/1860 [==============================] - 15s 8ms/step - loss: 0.8156 - accuracy: 0.6616 - val_loss: 0.8942 - val_accuracy: 0.6414\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f92b8365880>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "2ilvG8-Ie4my",
        "XqN3pmAk0G-7"
      ],
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}