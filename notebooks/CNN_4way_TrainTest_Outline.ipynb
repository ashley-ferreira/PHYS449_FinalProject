{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashley-ferreira/PHYS449_FinalProject/blob/main/notebooks/CNN_4way_TrainTest_Outline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PHYS 449: Final Project Notebook**\n",
        "#### Reproducing results from \"Morphological classification of galaxies with deep learning: comparing 3-way and 4-way CNNs\" by Mitchell K. Cavanagh, Kenji Bekki and Brent A. Groves\n",
        "\n",
        "*This all just assumed 4-way classification for now"
      ],
      "metadata": {
        "id": "oQfh4ENuykfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Packages**\n",
        "\n",
        "Begin by importing all the needed packages"
      ],
      "metadata": {
        "id": "FvlSR1N2ykL9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnQlrR-O-Jmh",
        "outputId": "4bc31aa3-4a23-4351-d0e6-621a32a47f8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING! APOGEE environment variable SDSS_LOCAL_SAS_MIRROR not set\n",
            "WARNING! Gaia environment variable GAIA_TOOLS_DATA not set\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, BatchNormalization, Flatten, Conv2D, MaxPool2D\n",
        "from keras.layers.core import Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define Network Structure**\n",
        "We are considering two 2D CNNs, C1 and C2, which are described in the paper and outlined below"
      ],
      "metadata": {
        "id": "Va-_oVtsykW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def C1(input_shape, unique_labels=4, dropout_rate=0.5):\n",
        "    '''\n",
        "    Defines the 2D Convolutional Neural Network (CNN) called C1\n",
        "    Parameters:    \n",
        "    \n",
        "        input_shape (arr): input shape for network\n",
        "        unique_labels (int): number unique labels \n",
        "        dropout_rate (float): dropout rate as fraction\n",
        "\n",
        "    Returns:\n",
        "        \n",
        "        model (keras model class): CNN to train\n",
        "    '''\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(filters=32, input_shape=input_shape, activation='relu', kernel_size=(5,5)))\n",
        "    model.add(Conv2D(filters=64, input_shape=input_shape, activation='relu', kernel_size=(5,5)))\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "\n",
        "    model.add(Dense(unique_labels, activation='softmax')) \n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "56ljipAIzmXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def C2(input_shape, unique_labels=2, dropout_rate=0.5):\n",
        "    '''\n",
        "    Defines the 2D Convolutional Neural Network (CNN) called C2\n",
        "    Parameters:    \n",
        "    \n",
        "        input_shape (arr): input shape for network\n",
        "        unique_labels (int): number unique labels \n",
        "        dropout_rate (float): dropout rate as fraction\n",
        "\n",
        "    Returns:\n",
        "        \n",
        "        model (keras model class): CNN to train\n",
        "    '''\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(filters=32, input_shape=input_shape, activation='relu', kernel_size=(7,7)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=64, input_shape=input_shape, activation='relu', kernel_size=(5,5)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(filters=64, input_shape=input_shape, activation='relu', kernel_size=(5,5)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters=128, input_shape=input_shape, activation='relu', kernel_size=(3,3)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "\n",
        "    model.add(Dense(unique_labels, activation='softmax')) \n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "lkSyV0tVz6OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyB4yRaICITu"
      },
      "source": [
        "# **Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmT2udOl-dlB",
        "outputId": "47c15d15-0b9e-4e8d-e174-e9d6b9223f42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Galaxy10.h5:  98%|█████████▊| 206M/210M [00:07<00:00, 44.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded Galaxy10 successfully to /root/.astroNN/datasets/Galaxy10.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGalaxy10.h5: 210MB [00:08, 24.0MB/s]                           \n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6yVoN5dCUtN"
      },
      "source": [
        "# **Sample Data**\n",
        "Here we check that the data files are how we expect them to be"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mg4pcdQQCLjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9eaQI4kCbVT"
      },
      "source": [
        "# **Split Data**\n",
        "Here we split data into trainng, testing datasets (validation split will be done by keras during training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWZ89HkS_kMK",
        "outputId": "38663d56-68c7-4d60-8c94-e29cffb0d06c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18517, 69, 69, 3)\n",
            "(18517, 10)\n"
          ]
        }
      ],
      "source": [
        "# splitting into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size = 0.15)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un1x2W4GAMex"
      },
      "source": [
        "# **Training**\n",
        "Ideally we use seperate notebooks to train each one\n",
        "\n",
        "C2 uses Adam, wheras C1 uses Adadelta: \n",
        "\n",
        "  https://www.aanda.org/articles/aa/full_html/2020/09/aa37963-20/aa37963-20.html\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "network_to_train = 'C1'\n",
        "\n",
        "# define hyperparameters of training\n",
        "if network_to_train == 'C1':\n",
        "  n_epochs = 13\n",
        "  # can't find learning rate mentioned so I'm leaving it as default for now\n",
        "  opt = optimizers.Adadelta()\n",
        "  cn_model = C1(X_train.shape[1:])\n",
        "elif network_to_train == 'C2':\n",
        "  n_epochs = 20\n",
        "  lr = 2*pow(10,-4)\n",
        "  opt = keras.optimizers.Adam(learning_rate=lr)\n",
        "  cn_model = C2(X_train.shape[1:])"
      ],
      "metadata": {
        "id": "Vcg_12Uh6ofB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show model architecture\n",
        "cn_model.summary()"
      ],
      "metadata": {
        "id": "gCWI5abdBa1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup W&B tracking \n"
      ],
      "metadata": {
        "id": "_M_9JE382TpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add early stopping (optional, if used set epochs to 100 as max)"
      ],
      "metadata": {
        "id": "W1pE38yD2dp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # train the model \n",
        "cn_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy', 'loss'])\n",
        "\n",
        "print('Model initialized and prepped, begin training...')\n",
        "classifier = cn_model.fit(X_train_1layer, y_train, epochs=n_epochs, validation_data=(X_test_1layer, y_test)) # fix, keep test seperate and use validation split"
      ],
      "metadata": {
        "id": "tvuJk18LAlsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "^ add specific batch data with keras? worse comes to worse we do it in pytorch but these articles seem helpful to get it going\n",
        "\n",
        "1.  https://meatba11.medium.com/keras-loading-and-processing-images-in-batches-1cff1b0f4aa4\n",
        "2. https://machinelearningmastery.com/how-to-load-large-datasets-from-directories-for-deep-learning-with-keras/\n",
        "3. https://stackoverflow.com/questions/61021025/split-data-into-batches\n",
        "4. https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
        "\n",
        "And more online, so I think we can figure it out\n"
      ],
      "metadata": {
        "id": "Q6jZTCtYDfGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot accuracy/loss versus epoch\n",
        "fig1 = plt.figure(figsize=(10,3))\n",
        "\n",
        "ax1 = plt.subplot(121)\n",
        "ax1.plot(classifier.history['accuracy'], color='darkslategray', linewidth=2, label='training')\n",
        "ax1.plot(classifier.history['val_accuracy'], linewidth=2, label='valiation') \n",
        "ax1.legend()\n",
        "ax1.set_title('Model Accuracy')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.set_xlabel('Epoch')\n",
        "\n",
        "ax2 = plt.subplot(122)\n",
        "ax2.plot(classifier.history['loss'], color='crimson', linewidth=2, label='training')\n",
        "ax2.plot(classifier.history['val_loss'], linewidth=2, label='validation')\n",
        "ax2.legend()\n",
        "ax2.set_title('Model Loss')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.set_xlabel('Epoch')\n",
        "\n",
        "fig1.savefig(model_dir_name +'/plots/'+'CNN_training_history.png')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WKDlFn3F1WxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing**\n",
        "Here we apply the model to the test set and create a confusion matrix to gauge performance"
      ],
      "metadata": {
        "id": "Ty5Ps1yI1a9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions on test set and compare to real labels\n",
        "preds_test = cn_model.predict(X_test, verbose=1)\n",
        "results = cn_model.evaluate(X_test, y_test) \n",
        "print(\"test loss, valid acc:\", results)"
      ],
      "metadata": {
        "id": "YaYRm0Xa1llA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot confusion matrix\n",
        "fig2 = plt.figure()\n",
        "cm = confusion_matrix(y_valid, preds_valid)\n",
        "plt.matshow(cm)\n",
        "\n",
        "for (i, j), z in np.ndenumerate(cm):\n",
        "    pyl.text(j, i, '{:0.1f}'.format(z), ha='center', va='center')\n",
        "plt.title('Confusion matrix (validation data)')\n",
        "plt.colorbar()\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.show()\n",
        "plt.savefig(model_dir_name +'plots/'+'CNN_confusion_matrix.png')"
      ],
      "metadata": {
        "id": "U1LAiOnE1pJQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}