{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **PHYS 449: Final Project Notebook**\n",
        "#### Reproducing results from \"Morphological classification of galaxies with deep learning: comparing 3-way and 4-way CNNs\" by Mitchell K. Cavanagh, Kenji Bekki and Brent A. Groves\n",
        "\n",
        "*This all just assumed 4-way classification for now"
      ],
      "metadata": {
        "id": "oQfh4ENuykfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set Current Working Directory**\n",
        "\n",
        "For example, for Ashley this is:\n",
        "\n",
        "'/content/drive/MyDrive/Fall 2022/PHYS 449/Final Project'"
      ],
      "metadata": {
        "id": "q-Ma_H5RnGtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CWD = '/content/drive/MyDrive/Fall 2022/PHYS 449/Final Project/'"
      ],
      "metadata": {
        "id": "7JMBwDQ4nV8N"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Packages**\n",
        "\n",
        "Begin by importing all the needed packages"
      ],
      "metadata": {
        "id": "FvlSR1N2ykL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if you've never used w&b before you need to run these commands, if this gives you issues just comment out w&b stuff for now\n",
        "#! pip install wandb\n",
        "#import wandb\n",
        "#wandb.login()"
      ],
      "metadata": {
        "id": "orwxGi_hFK2v"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "FnQlrR-O-Jmh"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, BatchNormalization, Flatten, Conv2D, MaxPool2D\n",
        "from keras.layers.core import Dropout\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "#DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# connect to w&b for experiment tracking\n",
        "#wandb.init(project=\"CNN-4way-C1-subset\", entity=\"449-final project\")"
      ],
      "metadata": {
        "id": "K4vEKvAuJDPC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define Network Structure**\n",
        "We are considering two 2D CNNs, C1 and C2, which are described in the paper and outlined below"
      ],
      "metadata": {
        "id": "Va-_oVtsykW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# need to double check but this is roughly right\n",
        "networkc1 = nn.Sequential(\n",
        "    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=3, padding=0),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=3, padding=0),\n",
        "    nn.ReLU(),\n",
        "    # max pool here\n",
        "    nn.MaxPool2d(kernel_size=2),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(1600, 256), \n",
        "    # dropout here\n",
        "    nn.Dropout(0.5),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256,4))"
      ],
      "metadata": {
        "id": "ZOk2Z6iXJF1y"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyB4yRaICITu"
      },
      "source": [
        "# **Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import google drive (need to put data folder as shortcut in your local drive My Drive):\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjvEFjidIlKu",
        "outputId": "116e1d83-61c1-414f-ca21-9031643bfa70"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ZmT2udOl-dlB"
      },
      "outputs": [],
      "source": [
        "#LOAD THE DATA FROM TXT FILE INTO A BATCH:\n",
        "def data_batch(datafile_index, num_images=10, data_file=CWD+'data/data_g_band.txt'):\n",
        "    '''\n",
        "    Description:\n",
        "        Access datafile.txt, each row is flattened 110x110 image + 1 label string (E, Sp, S0, Irr+Misc).\n",
        "        Returns an augmented batch of num_images X 40.\n",
        "        The labels are converted to 1D vectors (ex: Sp = [0,0,1,0])\n",
        "        Need to give a datafile_index that tells which rows to pick.\n",
        "    Inputs:\n",
        "        datafile_index: index of row in datafile to load. loads rows datafile_index to datafile_index+num_images.\n",
        "        num_images: number of different images to load per batch, total batch size \n",
        "        is 40 X num_images. (default: 10 (for 40X10 = 400 batch size like in paper)\n",
        "        data_file: datafile full path, need to add shortcut to local Drive. (default: '/content/drive/MyDrive/data/data_g_band.txt')\n",
        "    Outputs:\n",
        "        tensor_input_batch_aug: dimensions: (100, 100, num_images X 40). \n",
        "        tensor_label_batch_aug: dimensions: (num_images X 40, 4)\n",
        "    '''\n",
        "\n",
        "    #data_file = 'data_g_band.txt'\n",
        "\n",
        "    #Take batch of num_images rows from datafile:\n",
        "    with open(data_file, 'r') as f:\n",
        "        rows = f.readlines()[datafile_index:(datafile_index+num_images)]\n",
        "\n",
        "    #for batch size of 400 (augmented), need 10 images\n",
        "    data_batch = np.zeros((num_images,12101), dtype=np.dtype('U10'))\n",
        "    count = 0\n",
        "    for row in rows:\n",
        "        data_batch[count,:] = row.split()\n",
        "        count += 1\n",
        "\n",
        "    #separate label and input:\n",
        "    input_batch_flat = np.array(data_batch[:,:12100], dtype=int)\n",
        "    label_batch = np.array(data_batch[:,-1])\n",
        "\n",
        "    #convert input batch back to a 2D array:\n",
        "    input_batch = np.empty((110,110,np.shape(input_batch_flat)[0]), dtype=int)\n",
        "    for ii in range(np.shape(input_batch_flat)[0]):\n",
        "        input_batch[:,:,ii] = np.reshape(input_batch_flat[ii,:], (110,110))\n",
        "\n",
        "\n",
        "    #convert label batch into into 1D vector: \n",
        "    #E=0, S0=1, Sp=2, Irr+Misc=3\n",
        "    #ex: label = [0,0,1,0] ==> Sp galagy\n",
        "    arr_label_batch = np.empty((np.shape(label_batch)[0],4), dtype=int)\n",
        "\n",
        "    arr_label_batch[:,0] = np.array([label_batch == 'E'], dtype=int)\n",
        "    arr_label_batch[:,1] = np.array([label_batch == 'Sp'], dtype=int)\n",
        "    arr_label_batch[:,2] = np.array([label_batch == 'S0'], dtype=int)\n",
        "    arr_label_batch[:,3] = np.array([label_batch == 'Irr+Misc'], dtype=int)\n",
        "\n",
        "    #test with image plotted\n",
        "    #import matplotlib.pyplot as plt\n",
        "    #plt.imshow(input_batch[:,:,0])\n",
        "    #plt.show()\n",
        "\n",
        "    #NOW AUGMENT THE BATCH (40X more):\n",
        "    input_batch_aug = np.empty((100,100,np.shape(input_batch)[2]*40), dtype=int)\n",
        "    arr_label_batch_aug = np.empty((np.shape(arr_label_batch)[0]*40, 4), dtype=int)\n",
        "\n",
        "    count = 0\n",
        "    for ll in range(np.shape(input_batch)[2]):\n",
        "        #Crop 5X more image (100X100 pixels)\n",
        "        C1 = input_batch[:100,:100,ll]\n",
        "        C2 = input_batch[10:,:100,ll]\n",
        "        C3 = input_batch[:100,10:,ll]\n",
        "        C4 = input_batch[10:,10:,ll]\n",
        "        C5 = input_batch[5:105,5:105,ll]\n",
        "\n",
        "        C = [C1, C2, C3, C4, C5]\n",
        "\n",
        "        for kk in range(5):\n",
        "            #Rotate 4X more image (by 90 deg)\n",
        "            for jj in range(4):\n",
        "                C_R = np.rot90(C[kk], k=jj)\n",
        "                input_batch_aug[:,:,count] = C_R\n",
        "                arr_label_batch_aug[count,:] = arr_label_batch[ll,:]\n",
        "                count += 1\n",
        "                \n",
        "                input_batch_aug[:,:,count] = np.swapaxes(C_R,0,1)\n",
        "                arr_label_batch_aug[count,:] = arr_label_batch[ll,:]\n",
        "                count += 1\n",
        "\n",
        "\n",
        "    #PUT THE DATA AS A PYTORCH TENSOR:\n",
        "    tensor_input_batch_aug = torch.Tensor(input_batch_aug)\n",
        "    tensor_label_batch_aug = torch.Tensor(arr_label_batch_aug)\n",
        "    \n",
        "    return tensor_input_batch_aug, tensor_label_batch_aug\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6yVoN5dCUtN"
      },
      "source": [
        "# **Sample Data**\n",
        "Here we check that the data files are how we expect them to be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9eaQI4kCbVT"
      },
      "source": [
        "# **Split Data**\n",
        "Here we split data into trainng, testing datasets (validation split will be done by keras during training)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train and test set:\n",
        "rand_index = np.random.permutation(280)\n",
        "\n",
        "# MAKING AN ARTIFICIALLY SMALL DATASET SET FOR NOW\n",
        "rand_train = rand_index[:10]#[:200] #  does this do 200*50 for number of examples?\n",
        "rand_test = rand_index[10:20]#[200:] # valudation will be taken from test set"
      ],
      "metadata": {
        "id": "qj1M9LgXI5_R"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un1x2W4GAMex"
      },
      "source": [
        "# **Training**\n",
        "Ideally we use seperate notebooks to train each one\n",
        "\n",
        "C2 uses Adam, wheras C1 uses Adadelta: \n",
        "\n",
        "  https://www.aanda.org/articles/aa/full_html/2020/09/aa37963-20/aa37963-20.html\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "network_to_train = 'C1'\n",
        "\n",
        "# define hyperparameters of training\n",
        "if network_to_train == 'C1':\n",
        "  n_epochs = 13\n",
        "  # can't find learning rate mentioned so I'm leaving it as default for now\n",
        "  cn_model = networkc1\n",
        "  optimizer = torch.optim.Adadelta(cn_model.parameters())#, lr=2e-5)\n",
        "\n",
        "##### C2 HAS NOT BEEN IMPLIMENTED YET IN PYTORCH VERSION \n",
        "'''\n",
        "elif network_to_train == 'C2':\n",
        "  n_epochs = 20\n",
        "  lr = 2*pow(10,-4)\n",
        "  opt = keras.optimizers.Adam(learning_rate=lr)\n",
        "  cn_model = networkc2\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5cu8-Z52PSNH",
        "outputId": "a871d52a-e5d7-457b-c029-8d0f9c6064a3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nelif network_to_train == 'C2':\\n  n_epochs = 20\\n  lr = 2*pow(10,-4)\\n  opt = keras.optimizers.Adam(learning_rate=lr)\\n  cn_model = networkc2\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train in pytorch loop \n",
        "loss_fn = torch.nn.CrossEntropyLoss() # no need for softmax with this (just add for preds)\n",
        "\n",
        "# Initialize network & move to GPU\n",
        "#cn_model.to(DEVICE)  # comment out if this gives you issues\n",
        "\n",
        "# For monitoring losses\n",
        "avg_epoch_losses_train = []\n",
        "avg_epoch_losses_val = []\n",
        "\n",
        "\n",
        "batch_size = np.shape(rand_train)[0]\n",
        "valid_split = 0.1\n",
        "\n",
        "print('Model initialized and prepped, begin training...')\n",
        "cn_model.train()\n",
        "for epoch in range(n_epochs):  # progress bar\n",
        "    print('epoch:', epoch+1)\n",
        "\n",
        "    # quick fix to get dataset size\n",
        "    ds_size = 0\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for ii in range(batch_size): \n",
        "      print('batch', ii+1, '/', batch_size)\n",
        "      im, y = data_batch(datafile_index=50*rand_train[ii], num_images=batch_size)\n",
        "\n",
        "      # reshaping im to what we want (can do this as data output too)\n",
        "      im = im.reshape(im.shape[2], 1, 100, 100)\n",
        "\n",
        "      y_pred = cn_model(im)\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      #print(loss)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      epoch_loss += loss.item()\n",
        "      ds_size += 1\n",
        "\n",
        "    t_loss = epoch_loss / ds_size\n",
        "    print('training loss:', t_loss)\n",
        "    avg_epoch_losses_train.append(t_loss)\n",
        "\n",
        "    t_acc = torch.sum(y_pred == y).numpy()/(4*ds_size)\n",
        "    print('training accuracy:', t_acc)\n",
        "    avg_epoch_acc_train.append(t_acc)\n",
        "\n",
        "    # Validation\n",
        "    cn_model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "      # add unaugmented stuff for test and maybe validation? below is a quick fix\n",
        "      for ii in range(np.shape(rand_test)[0]):\n",
        "        if ii == 0:\n",
        "          im_valid, y_valid = data_batch(datafile_index=50*rand_test[ii], num_images=50)\n",
        "          im_valid = im_valid.reshape(im_valid.shape[2], 1, 100, 100)\n",
        "        else:\n",
        "          pass\n",
        "      y_pred_valid = cn_model(im_valid)\n",
        "      loss = loss_fn(y_pred_valid, y_valid)\n",
        "      epoch_loss += loss.item()\n",
        "      v_loss = epoch_loss\n",
        "      avg_epoch_losses_val.append(v_loss)\n",
        "      print('validation loss:', v_loss)\n",
        "\n",
        "      v_acc = torch.sum(y_pred_valid == y_valid).numpy()/(4*ds_size)\n",
        "      print('validation accuracy:', v_acc)\n",
        "      vg_epoch_acc_train.append(v_acc)\n",
        "\n",
        "      #wandb.log({\"loss\": loss, \"validation_loss\": v_loss})\n",
        "      #wandb.watch(cn_model)\n",
        "\n",
        "# to do: add in C2, add in accuracy calculations and tracking, change variable names and all to get plotting below working"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "12RTbYBaK-wj",
        "outputId": "a17aebe2-908f-4e26-a62f-12a0da1f0790"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized and prepped, begin training...\n",
            "epoch: 1\n",
            "batch 1 / 10\n",
            "batch 2 / 10\n",
            "batch 3 / 10\n",
            "batch 4 / 10\n",
            "batch 5 / 10\n",
            "batch 6 / 10\n",
            "batch 7 / 10\n",
            "batch 8 / 10\n",
            "batch 9 / 10\n",
            "batch 10 / 10\n",
            "training loss: 1.1821406722068786\n",
            "training accuracy: 0.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-baaf6e23f8bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mt_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mds_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mavg_epoch_acc_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'avg_epoch_acc_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model itself \n",
        "torch.save(cn_model.state_dict(), 'test_model1')#, CWD + 'Notebooks/models/')"
      ],
      "metadata": {
        "id": "4-K30VyfjWYU"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot accuracy/loss versus epoch\n",
        "fig1 = plt.figure(figsize=(10,3))\n",
        "\n",
        "'''\n",
        "ax1 = plt.subplot(121)\n",
        "ax1.plot(classifier.history['accuracy'], '--', color='darkslategray', linewidth=2, label='training')\n",
        "ax1.plot(classifier.history['val_accuracy'], linewidth=2, label='valiation') \n",
        "ax1.legend()\n",
        "ax1.set_title('Model Accuracy')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.set_xlabel('Epoch')\n",
        "'''\n",
        "\n",
        "ax2 = plt.subplot(122)\n",
        "ax2.plot(avg_epoch_losses_train, '--', color='crimson', linewidth=2, label='training')\n",
        "ax2.plot(avg_epoch_losses_val, linewidth=2, label='validation')\n",
        "ax2.legend()\n",
        "ax2.set_title('Model Loss')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.set_xlabel('Epoch')\n",
        "\n",
        "fig1.savefig(CWD + 'Notebooks/plots/'+'CNN_training_history.png')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WKDlFn3F1WxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing**\n",
        "Here we apply the model to the test set and create a confusion matrix to gauge performance"
      ],
      "metadata": {
        "id": "Ty5Ps1yI1a9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### ADAPT THIS TO GET TEST SET STATS (way to make it not do data augmentation and just stick with original images?)\n",
        "for ii in range(np.shape(rand_test)[0]):\n",
        "  if ii == 0:\n",
        "    pass\n",
        "  else:\n",
        "    image_batch, label_batch = data_batch(datafile_index=50*rand_test[ii], num_images=50)"
      ],
      "metadata": {
        "id": "yEGmwZciDo13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions on test set and compare to real labels\n",
        "preds_test = cn_model.predict(X_test, verbose=1)\n",
        "results = cn_model.evaluate(X_test, y_test) \n",
        "print(\"test loss, valid acc:\", results)"
      ],
      "metadata": {
        "id": "YaYRm0Xa1llA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot confusion matrix\n",
        "fig2 = plt.figure()\n",
        "cm = confusion_matrix(y_valid, preds_valid)\n",
        "plt.matshow(cm)\n",
        "\n",
        "for (i, j), z in np.ndenumerate(cm):\n",
        "    pyl.text(j, i, '{:0.1f}'.format(z), ha='center', va='center')\n",
        "plt.title('Confusion matrix (validation data)')\n",
        "plt.colorbar()\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.show()\n",
        "plt.savefig(model_dir_name +'plots/'+'CNN_confusion_matrix.png')"
      ],
      "metadata": {
        "id": "U1LAiOnE1pJQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}