{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashley-ferreira/PHYS449_FinalProject/blob/main/notebooks/KerasC1_our_data_40x_trial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z41xLe93yrK8",
        "outputId": "3d09ccff-79e7-4474-ae64-29732d5368da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting astroNN\n",
            "  Downloading astroNN-1.0.1.tar.gz (9.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.3 MB 16.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from astroNN) (1.21.6)\n",
            "Requirement already satisfied: astropy in /usr/local/lib/python3.8/dist-packages (from astroNN) (4.3.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from astroNN) (3.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from astroNN) (3.2.2)\n",
            "Collecting astroquery\n",
            "  Downloading astroquery-0.4.6-py3-none-any.whl (4.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5 MB 76.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from astroNN) (1.3.5)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.8/dist-packages (from astroNN) (0.11.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from astroNN) (1.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from astroNN) (4.64.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from astroNN) (21.3)\n",
            "Requirement already satisfied: pyerfa>=1.7.3 in /usr/local/lib/python3.8/dist-packages (from astropy->astroNN) (2.0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.3.2 in /usr/local/lib/python3.8/dist-packages (from astroquery->astroNN) (4.6.3)\n",
            "Requirement already satisfied: requests>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from astroquery->astroNN) (2.23.0)\n",
            "Requirement already satisfied: html5lib>=0.999 in /usr/local/lib/python3.8/dist-packages (from astroquery->astroNN) (1.0.1)\n",
            "Collecting keyring>=4.0\n",
            "  Downloading keyring-23.11.0-py3-none-any.whl (36 kB)\n",
            "Collecting pyvo>=1.1\n",
            "  Downloading pyvo-1.4-py3-none-any.whl (885 kB)\n",
            "\u001b[K     |████████████████████████████████| 885 kB 80.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from html5lib>=0.999->astroquery->astroNN) (0.5.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from html5lib>=0.999->astroquery->astroNN) (1.15.0)\n",
            "Collecting jeepney>=0.4.2\n",
            "  Downloading jeepney-0.8.0-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 6.6 MB/s \n",
            "\u001b[?25hCollecting jaraco.classes\n",
            "  Downloading jaraco.classes-3.2.3-py3-none-any.whl (6.0 kB)\n",
            "Collecting SecretStorage>=3.2\n",
            "  Downloading SecretStorage-3.3.3-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.11.4 in /usr/local/lib/python3.8/dist-packages (from keyring>=4.0->astroquery->astroNN) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.11.4->keyring>=4.0->astroquery->astroNN) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.4.3->astroquery->astroNN) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.4.3->astroquery->astroNN) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.4.3->astroquery->astroNN) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.4.3->astroquery->astroNN) (3.0.4)\n",
            "Collecting cryptography>=2.0\n",
            "  Downloading cryptography-38.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 66.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.8/dist-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring>=4.0->astroquery->astroNN) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring>=4.0->astroquery->astroNN) (2.21)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from jaraco.classes->keyring>=4.0->astroquery->astroNN) (9.0.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->astroNN) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->astroNN) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->astroNN) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->astroNN) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->astroNN) (2022.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->astroNN) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->astroNN) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->astroNN) (1.7.3)\n",
            "Building wheels for collected packages: astroNN\n",
            "  Building wheel for astroNN (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for astroNN: filename=astroNN-1.0.1-py3-none-any.whl size=9284593 sha256=92cdda0616bd9b9795bc257cfd94e3213c14c6fcff0d978824984c0a5d156ebd\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/b6/1f/222aea123a5de8a34c3dd95bb73dca35e342ef3901328e9db0\n",
            "Successfully built astroNN\n",
            "Installing collected packages: jeepney, cryptography, SecretStorage, jaraco.classes, pyvo, keyring, astroquery, astroNN\n",
            "Successfully installed SecretStorage-3.3.3 astroNN-1.0.1 astroquery-0.4.6 cryptography-38.0.4 jaraco.classes-3.2.3 jeepney-0.8.0 keyring-23.11.0 pyvo-1.4\n"
          ]
        }
      ],
      "source": [
        "!pip install astroNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yMvvMay_bgma"
      },
      "outputs": [],
      "source": [
        "#from astroNN.datasets import galaxy10\n",
        "#from astroNN.datasets.galaxy10 import galaxy10cls_lookup\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0L6m3MIYz_p",
        "outputId": "66544d18-8c55-4336-9895-e3c8ec788fd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Import google drive (need to put data folder as shortcut in your local drive My Drive):\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQRml3VHevfV"
      },
      "source": [
        "# **Full data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dh5a6h6wY-UM"
      },
      "outputs": [],
      "source": [
        "#LOAD THE DATA FROM TXT FILE INTO A BATCH:\n",
        "def data_batch(datafile_index, num_images, data_file='MyDrive/data_g_band_v2.txt', plotting=False):\n",
        "    '''\n",
        "    Description:\n",
        "        Access datafile.txt, each row is flattened 110x110 image + 1 label string (E, Sp, S0, Irr+Misc).\n",
        "        Returns an augmented batch of num_images X 40.\n",
        "        The labels are converted to 1D vectors (ex: Sp = [0,0,1,0])\n",
        "        Need to give a datafile_index that tells which rows to pick.\n",
        "    Inputs:\n",
        "        datafile_index: index of row in datafile to load. loads rows datafile_index to datafile_index+num_images.\n",
        "        num_images: number of different images to load per batch, total batch size \n",
        "        is 40 X num_images. (default: 10 (for 40X10 = 400 batch size like in paper)\n",
        "        data_file: datafile full path, need to add shortcut to local Drive. (default: '/content/drive/MyDrive/data/data_g_band.txt')\n",
        "    Outputs:\n",
        "        tensor_input_batch_aug: dimensions: (100, 100, num_images X 40).\n",
        "        tensor_label_batch_aug: dimensions: (num_images X 40, 4)\n",
        "    '''\n",
        "\n",
        "    #Take batch of num_images rows from datafile:\n",
        "    with open(data_file, 'r') as f:\n",
        "        rows = f.readlines()[datafile_index:(datafile_index+num_images)]\n",
        "\n",
        "    #for batch size of 400 (augmented), need 10 images\n",
        "    data_batch = np.zeros((num_images,12101), dtype=np.dtype('U10'))\n",
        "    count = 0\n",
        "    for row in rows:\n",
        "        data_batch[count,:] = row.split()\n",
        "        count += 1\n",
        "\n",
        "    #separate label and input:\n",
        "    input_batch_flat = np.array(data_batch[:,:12100], dtype=float)#, dtype=int)\n",
        "    label_batch = np.array(data_batch[:,-1])\n",
        "\n",
        "    #convert input batch back to a 2D array:\n",
        "    input_batch = np.zeros((110,110,np.shape(input_batch_flat)[0]))#, dtype=int)\n",
        "    for ii in range(np.shape(input_batch_flat)[0]):\n",
        "        input_batch[:,:,ii] = np.reshape(input_batch_flat[ii,:], (110,110))\n",
        "\n",
        "\n",
        "    #convert label batch into into 1D vector: \n",
        "    #E=0, S0=1, Sp=2, Irr+Misc=3\n",
        "    #ex: label = [0,0,1,0] ==> Sp galagy\n",
        "    arr_label_batch = np.zeros((np.shape(label_batch)[0],4), dtype=int)\n",
        "    arr_label_batch[:,0] = np.array([label_batch == 'E'], dtype=int)\n",
        "    arr_label_batch[:,1] = np.array([label_batch == 'Sp'], dtype=int)\n",
        "    arr_label_batch[:,2] = np.array([label_batch == 'S0'], dtype=int)\n",
        "    arr_label_batch[:,3] = np.array([label_batch == 'Irr+Misc'], dtype=int)\n",
        "\n",
        "    if plotting == True:\n",
        "      #test with image plotted\n",
        "      plt.imshow(input_batch[:,:,0])\n",
        "      plt.show()\n",
        "\n",
        "    #NOW AUGMENT THE BATCH (40X more):\n",
        "    input_batch_aug = np.empty((100,100,np.shape(input_batch)[2]*40), dtype=int)\n",
        "    arr_label_batch_aug = np.empty((np.shape(arr_label_batch)[0]*40, 4), dtype=int)\n",
        "\n",
        "    count = 0\n",
        "    for ll in range(np.shape(input_batch)[2]):\n",
        "        #Crop 5X more image (100X100 pixels)\n",
        "        C1 = input_batch[:100,:100,ll]\n",
        "        C2 = input_batch[10:,:100,ll]\n",
        "        C3 = input_batch[:100,10:,ll]\n",
        "        C4 = input_batch[10:,10:,ll]\n",
        "        C5 = input_batch[5:105,5:105,ll]\n",
        "\n",
        "        C = [C1, C2, C3, C4, C5]\n",
        "\n",
        "        for kk in range(5):\n",
        "            #Rotate 4X more image (by 90 deg)\n",
        "            for jj in range(4):\n",
        "                C_R = np.rot90(C[kk], k=jj)\n",
        "                input_batch_aug[:,:,count] = C_R\n",
        "                arr_label_batch_aug[count,:] = arr_label_batch[ll,:]\n",
        "                count += 1\n",
        "                \n",
        "                input_batch_aug[:,:,count] = np.swapaxes(C_R,0,1)\n",
        "                arr_label_batch_aug[count,:] = arr_label_batch[ll,:]\n",
        "                count += 1\n",
        "\n",
        "\n",
        "    #PUT THE DATA AS A PYTORCH TENSOR:\n",
        "    #tensor_input_batch_aug = torch.Tensor(input_batch_aug)\n",
        "    #tensor_label_batch_aug = torch.Tensor(arr_label_batch_aug)\n",
        "    \n",
        "    return input_batch_aug, arr_label_batch_aug"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ilvG8-Ie4my"
      },
      "source": [
        "# **Our augmented data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9czaV48Ge4eq"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Z-BooKMxZIt0"
      },
      "outputs": [],
      "source": [
        "# #LESS DATA AUGMENTION: crop only = X5 augmentation:\n",
        "\n",
        "# #AUGMENT ONLY X5 (ONLY BY CROPPING)\n",
        "# def data_batch_aug5(datafile_index, num_images=10,  data_file='/content/drive/MyDrive/data_g_band.txt', plotting=False):\n",
        "#     '''\n",
        "#     Description:\n",
        "#         Access datafile.txt, each row is flattened 110x110 image + 1 label string (E, Sp, S0, Irr+Misc).\n",
        "#         Returns an augmented batch of num_images X 5.\n",
        "#         The labels are converted to 1D vectors (ex: Sp = [0,0,1,0])\n",
        "#         Need to give a datafile_index that tells which rows to pick.\n",
        "#     Inputs:\n",
        "#         datafile_index: index of row in datafile to load. loads rows datafile_index to datafile_index+num_images.\n",
        "#         num_images: number of different images to load per batch, total batch size \n",
        "#         is 5 X num_images. (default: 10 (for 5X10 = 400 batch size like in paper)\n",
        "#         data_file: datafile full path, need to add shortcut to local Drive. (default: '/content/drive/MyDrive/data/data_g_band.txt')\n",
        "#     Outputs:\n",
        "#         tensor_input_batch_aug: dimensions: (100, 100, num_images X 5). \n",
        "#         tensor_label_batch_aug: dimensions: (num_images X 5, 4)\n",
        "#     '''\n",
        "\n",
        "#     #data_file = 'data_g_band.txt'\n",
        "\n",
        "#     #Take batch of num_images rows from datafile:\n",
        "#     with open(data_file, 'r') as f:\n",
        "#         rows = f.readlines()[datafile_index:(datafile_index+num_images)]\n",
        "\n",
        "#     #for batch size of 400 (augmented), need 10 images\n",
        "#     data_batch = np.zeros((num_images,12101), dtype=np.dtype('U10'))\n",
        "#     count = 0\n",
        "#     for row in rows:\n",
        "#         data_batch[count,:] = row.split()\n",
        "#         count += 1\n",
        "\n",
        "#     #separate label and input:\n",
        "#     input_batch_flat = np.array(data_batch[:,:12100], dtype=int)\n",
        "#     label_batch = np.array(data_batch[:,-1])\n",
        "\n",
        "#     #convert input batch back to a 2D array:\n",
        "#     input_batch = np.empty((110,110,np.shape(input_batch_flat)[0]), dtype=int)\n",
        "#     for ii in range(np.shape(input_batch_flat)[0]):\n",
        "#         input_batch[:,:,ii] = np.reshape(input_batch_flat[ii,:], (110,110))\n",
        "\n",
        "\n",
        "#     #convert label batch into into 1D vector: \n",
        "#     #E=0, S0=1, Sp=2, Irr+Misc=3\n",
        "#     #ex: label = [0,0,1,0] ==> Sp galagy\n",
        "#     arr_label_batch = np.empty((np.shape(label_batch)[0],4), dtype=int)\n",
        "#     arr_label_batch[:,0] = np.array([label_batch == 'E'], dtype=int)\n",
        "#     arr_label_batch[:,1] = np.array([label_batch == 'Sp'], dtype=int)\n",
        "#     arr_label_batch[:,2] = np.array([label_batch == 'S0'], dtype=int)\n",
        "#     arr_label_batch[:,3] = np.array([label_batch == 'Irr+Misc'], dtype=int)\n",
        "\n",
        "#     #test with image plotted\n",
        "#     if plotting == True:\n",
        "#       plt.imshow(input_batch[:,:,0])\n",
        "#       plt.show()\n",
        "\n",
        "#     #NOW AUGMENT THE BATCH (5X more):\n",
        "#     how_much_augment = 5\n",
        "#     input_batch_aug = np.empty((100,100,np.shape(input_batch)[2]*how_much_augment), dtype=int)\n",
        "#     arr_label_batch_aug = np.empty((np.shape(arr_label_batch)[0]*how_much_augment, 4), dtype=int)\n",
        "\n",
        "#     count = 0\n",
        "#     for ll in range(np.shape(input_batch)[2]):\n",
        "#         #Crop 5X more image (100X100 pixels)\n",
        "#         C1 = input_batch[:100,:100,ll]\n",
        "#         C2 = input_batch[10:,:100,ll]\n",
        "#         C3 = input_batch[:100,10:,ll]\n",
        "#         C4 = input_batch[10:,10:,ll]\n",
        "#         C5 = input_batch[5:105,5:105,ll]\n",
        "\n",
        "#         C = [C1, C2, C3, C4, C5]\n",
        "\n",
        "#         for kk in range(5):\n",
        "#             input_batch_aug[:,:,count] = C[kk]\n",
        "#             arr_label_batch_aug[count,:] = arr_label_batch[ll,:]\n",
        "#             count += 1\n",
        "\n",
        "#     #PUT THE DATA AS A PYTORCH TENSOR:\n",
        "#     #tensor_input_batch_aug = torch.Tensor(input_batch_aug)\n",
        "#     #tensor_label_batch_aug = torch.Tensor(arr_label_batch_aug)\n",
        "    \n",
        "#     return input_batch_aug, arr_label_batch_aug\n",
        "\n",
        "# '''\n",
        "# #Test above function:\n",
        "# rand_index = np.random.permutation(1403) #10 images\n",
        "# rand_train = rand_index[:200] #arbitrary values\n",
        "# rand_test = rand_index[200:]\n",
        "\n",
        "# #Use this loop for training over entire dataset at each epochs\n",
        "# for ii in range(np.shape(rand_train)[0]):\n",
        "#   image_batch, label_batch = data_batch_aug5(datafile_index=10*rand_train[ii], num_images=10)\n",
        "#   ##print(np.shape(image_batch))\n",
        "#   ##print(np.shape(label_batch))\n",
        "#   ##print(label_batch)\n",
        "#   #Check: 10 images X 5 augmentation = 100 x 100 x 50 tensor size\n",
        "#   #check: label size is 10 x 5 = 50 x 4 (4 labels)\n",
        "#   #check: label is 5 type in a row then another 5 in a row etc ...\n",
        "# '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbjMNWI2v13Z"
      },
      "source": [
        "# **Get data for a select index**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc9NLJ2EfKW7",
        "outputId": "1a2e3b7a-da99-4cda-f541-83c6aab85821"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(100, 100, 300000)\n"
          ]
        }
      ],
      "source": [
        "input_batch_aug, arr_label_batch_aug = data_batch(datafile_index=0, num_images=7500,  data_file='/content/drive/MyDrive/data_g_band_v2.txt', plotting=False)\n",
        "print(input_batch_aug.shape)\n",
        "len_input = len(input_batch_aug.T)\n",
        "#input_batch_aug.reshape((1, 100, 100, 9000))\n",
        "#print(input_batch_aug.shape)\n",
        "#print(arr_label_batch_aug.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHlAUKTmTZiq",
        "outputId": "b26028f2-e89b-4725-c984-6e029cea6bc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 100, 100, 300000)\n"
          ]
        }
      ],
      "source": [
        "#print(input_batch_aug.shape)\n",
        "input_batch_aug_final = input_batch_aug.reshape(1, 100, 100, len_input)\n",
        "print(input_batch_aug_final.shape)\n",
        "#print(arr_label_batch_aug.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gXSPJ_czWlz"
      },
      "source": [
        "# **Data split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eget-kbzzWrm",
        "outputId": "950a629c-a4c5-4658-ebda-fa2b3e97af0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(255000, 100, 100, 1)\n",
            "(255000, 4)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(input_batch_aug_final.T, arr_label_batch_aug, test_size = 0.15)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlivDtAJzayP"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE7j0xKEza4E",
        "outputId": "3d1c984c-8f97-4f42-b146-a85ca30b621e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 96, 96, 32)        832       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 92, 92, 64)        51264     \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 46, 46, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 135424)            0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 135424)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               34668800  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 1028      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 34,721,924\n",
            "Trainable params: 34,721,924\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(filters=32, input_shape=(100,100,1), activation='relu', kernel_size=(5,5)))\n",
        "model.add(Conv2D(filters=64, input_shape=(100,100,1), activation='relu', kernel_size=(5,5)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(4, activation='softmax')) \n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjNjh8lC1NzY",
        "outputId": "2b43a3be-7aea-48a2-e2d7-3efd5335ff4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "6774/6774 [==============================] - 85s 10ms/step - loss: 1.7732 - accuracy: 0.4589 - val_loss: 1.2035 - val_accuracy: 0.3412\n",
            "Epoch 2/20\n",
            "6774/6774 [==============================] - 62s 9ms/step - loss: 1.0763 - accuracy: 0.5348 - val_loss: 1.1285 - val_accuracy: 0.4544\n",
            "Epoch 3/20\n",
            "6774/6774 [==============================] - 62s 9ms/step - loss: 1.0315 - accuracy: 0.5557 - val_loss: 1.0813 - val_accuracy: 0.5215\n",
            "Epoch 4/20\n",
            "6774/6774 [==============================] - 62s 9ms/step - loss: 1.0054 - accuracy: 0.5702 - val_loss: 1.0483 - val_accuracy: 0.5397\n",
            "Epoch 5/20\n",
            "6774/6774 [==============================] - 62s 9ms/step - loss: 0.9811 - accuracy: 0.5842 - val_loss: 1.0124 - val_accuracy: 0.5567\n",
            "Epoch 6/20\n",
            "6774/6774 [==============================] - 62s 9ms/step - loss: 0.9607 - accuracy: 0.5971 - val_loss: 0.9803 - val_accuracy: 0.5787\n",
            "Epoch 7/20\n",
            "6774/6774 [==============================] - 61s 9ms/step - loss: 0.9417 - accuracy: 0.6075 - val_loss: 0.9452 - val_accuracy: 0.6033\n",
            "Epoch 8/20\n",
            "6774/6774 [==============================] - 61s 9ms/step - loss: 0.9271 - accuracy: 0.6156 - val_loss: 0.9280 - val_accuracy: 0.6104\n",
            "Epoch 9/20\n",
            "6774/6774 [==============================] - 61s 9ms/step - loss: 0.9137 - accuracy: 0.6253 - val_loss: 0.9138 - val_accuracy: 0.6184\n",
            "Epoch 10/20\n",
            "6774/6774 [==============================] - 61s 9ms/step - loss: 0.9006 - accuracy: 0.6308 - val_loss: 0.8951 - val_accuracy: 0.6284\n",
            "Epoch 11/20\n",
            "6774/6774 [==============================] - 62s 9ms/step - loss: 0.8913 - accuracy: 0.6344 - val_loss: 0.8747 - val_accuracy: 0.6420\n",
            "Epoch 12/20\n",
            "6774/6774 [==============================] - 61s 9ms/step - loss: 0.8822 - accuracy: 0.6400 - val_loss: 0.8657 - val_accuracy: 0.6456\n",
            "Epoch 13/20\n",
            "6774/6774 [==============================] - 61s 9ms/step - loss: 0.8729 - accuracy: 0.6457 - val_loss: 0.8619 - val_accuracy: 0.6446\n",
            "Epoch 14/20\n",
            "6774/6774 [==============================] - 61s 9ms/step - loss: 0.8649 - accuracy: 0.6486 - val_loss: 0.8574 - val_accuracy: 0.6458\n",
            "Epoch 15/20\n",
            "6774/6774 [==============================] - 61s 9ms/step - loss: 0.8573 - accuracy: 0.6518 - val_loss: 0.8396 - val_accuracy: 0.6567\n",
            "Epoch 16/20\n",
            "6774/6774 [==============================] - 62s 9ms/step - loss: 0.8503 - accuracy: 0.6549 - val_loss: 0.8330 - val_accuracy: 0.6609\n",
            "Epoch 17/20\n",
            "6774/6774 [==============================] - 62s 9ms/step - loss: 0.8449 - accuracy: 0.6574 - val_loss: 0.8306 - val_accuracy: 0.6599\n",
            "Epoch 18/20\n",
            "6774/6774 [==============================] - 62s 9ms/step - loss: 0.8389 - accuracy: 0.6607 - val_loss: 0.8201 - val_accuracy: 0.6659\n",
            "Epoch 19/20\n",
            "6774/6774 [==============================] - 62s 9ms/step - loss: 0.8334 - accuracy: 0.6642 - val_loss: 0.8114 - val_accuracy: 0.6703\n",
            "Epoch 20/20\n",
            "6774/6774 [==============================] - 62s 9ms/step - loss: 0.8275 - accuracy: 0.6654 - val_loss: 0.8033 - val_accuracy: 0.6753\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcd0d744910>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimizer = tf.keras.optimizers.Adadelta(learning_rate=0.0002)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=20, validation_split=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "H0psMHZNZTbw",
        "outputId": "38593f66-1c85-471d-9360-02ffb635adae"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f6aab92db4c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training  Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'loss'"
          ]
        }
      ],
      "source": [
        "plt.plot(model.history.history['loss'],color='b', label='Training Loss')\n",
        "plt.plot(model.history.history['val_loss'],color='r', label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.plot(model.history.history['accuracy'],color='b', label='Training  Accuracy')\n",
        "plt.plot(model.history.history['val_accuracy'],color='r', label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pBG-R8XZvps"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 10,10\n",
        "rcParams.update({'font.size': 15})\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "predictions = (predictions > 0.5) \n",
        "labels = ['E', 'S0', 'Sp', 'Irr']\n",
        "\n",
        "cm = confusion_matrix(y_test.argmax(axis=1), predictions.argmax(axis=1), normalize='true')\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('C1 Confusion Matrix')\n",
        "#plt.facecolor(\"white\")\n",
        "plt.show()\n",
        "\n",
        "results = model.evaluate(X_test, y_test)\n",
        "print(\"C1 --> test loss, test acc:\", results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "2ilvG8-Ie4my",
        "XqN3pmAk0G-7"
      ],
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.9.11 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "f8696d3d898bc62b90f4cd7d878fc87a051ec77bbff0b42338ab50969b9d3714"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
